{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqlARsMhuFVN"
   },
   "source": [
    "**Setup. Import required libraries and helper functions. Load our data.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cshjQiC4Vpd",
    "outputId": "5b6f1ed6-22df-41c3-fabe-baae1b7e7825"
   },
   "outputs": [],
   "source": [
    "import os     # Used to sort files for file reading\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re     # Regex, used in the file reading\n",
    "\n",
    "import cv2          # Open Computer Vision 2 - a must for any image manipulation\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  # Other staples for working with images\n",
    "import random\n",
    "import copy\n",
    "from skimage.util import random_noise\n",
    "\n",
    "import torch\n",
    "from torch import nn    # Pytorch\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, sampler\n",
    "\n",
    "import dsntnn        # import library with DSNT layer\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "import scipy.misc       # Don't remember what this is used for - may not be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFVyE6cR9EuZ",
    "outputId": "cc9f1c4b-2f00-49b3-ede0-ed6663940d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CS_6955_Project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone our labeled images stored in GitHub\n",
    "\n",
    "!git clone https://github.com/Hunterdjensen/CS_6955_Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sH3s_tpoA2nz",
    "outputId": "74bac8ee-463b-4014-cc4a-0aaa68298331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Determine what type of device we are using\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X3FByR_uo02p"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Set Constants\n",
    "# \n",
    "\n",
    "PATH_TO_IMAGE_FILES = 'CS_6955_Project/Examples'\n",
    "CLASSIFICATION_RESULTS_FILENAME = 'classification_results.txt'\n",
    "\n",
    "TEMPLATE_FILE_PATH = '/content/CS_6955_Project/Template/template_tight.png' # Used when generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HYxuXur3Nhcw"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helper functions for parsing the images\n",
    "#\n",
    "\n",
    "def line_to_points(line):\n",
    "    \"\"\"\n",
    "    Converts an input line into a list of points\n",
    "    \"\"\"\n",
    "    points_ascii = line.split(\"\\t\")[2]  # The 3rd column contains points data\n",
    "    points = eval(points_ascii)         # Convert the string into a list\n",
    "    points = np.array(points)           # List to numpy array of shape [num_cards, 4, 2]\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_num_cards(line):\n",
    "    return int(line.split(\"\\t\")[1])\n",
    "\n",
    "\n",
    "def np_to_tuple(point):\n",
    "    '''\n",
    "    Most cv2 functions take coordinates in as a tuple, not an array with 2 elements\n",
    "    so this function can be called to turn np array of shape [2] into a tuple\n",
    "    '''\n",
    "    return (point[0], point[1])\n",
    "\n",
    "\n",
    "def displayRGB(image, points=None):\n",
    "    \"\"\"\n",
    "    Displays an image and its coordinate points (if given) for pictures in \n",
    "    RGB format.\n",
    "    \"\"\"\n",
    "    temp_img = image.copy()\n",
    "    if points is not None:\n",
    "        for j, frame in enumerate(points):\n",
    "            for i, point in enumerate(frame):\n",
    "                cv2.line(temp_img, np_to_tuple(point), np_to_tuple(points[j][i - 1]), (255, 0, 0), thickness=2)\n",
    "    plt.imshow(temp_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def expand_image(img, points, size=225):\n",
    "    \"\"\"\n",
    "    Our network architechture is designed for square inputs.\n",
    "    This function will perform the necessary scaling to get an input into the right format.\n",
    "    \"\"\"\n",
    "    height, width, _ = img.shape\n",
    "    points_oob = 0\n",
    "\n",
    "    # Check how many points are out of bounds\n",
    "    if points is not None:\n",
    "      for card in points:\n",
    "        for point in card:\n",
    "          if (point[0] < 0) or (point[0] >= width) or (point[1] < 0) or (point[1] >= height):\n",
    "            points_oob += 1\n",
    "    \n",
    "    # If picture is too big, scaled it down to the size\n",
    "    if (height > width):\n",
    "        if (height > size):\n",
    "            img = imutils.resize(img, height=size)\n",
    "    else:\n",
    "        if (width > size):\n",
    "            img = imutils.resize(img, width=size)\n",
    "\n",
    "    # Regrab the new height and width\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # If no points were passed in, assume the corners of the image\n",
    "    if points is None:\n",
    "        points = np.array([[[0, height], [0, 0], [width, 0], [width, height]]])\n",
    "\n",
    "    # Add border (if the image isn't already square)\n",
    "    left = int((size - width) / 2)  # So if image has width of 125, then left is 100/2=50\n",
    "    right = int(size - (left+width))  # Remainder\n",
    "    top = int((size - height) / 2)\n",
    "    bottom = int(size - (top+height))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\n",
    "    # Update points\n",
    "    for i, card in enumerate(points):      # The first dim of 'points' is each card\n",
    "        for j, point in enumerate(card):   # Second dimension is a list of 4 tuples (each corner point)\n",
    "            points[i][j] = [point[0]+left, point[1]+top]    # Shift it to match the new image\n",
    "\n",
    "    return (img, points, points_oob)\n",
    "\n",
    "\n",
    "def read_images(max_samples=50000, \n",
    "                cards_per_image=1, \n",
    "                yellow_borders_only=True, \n",
    "                digital_images=False, \n",
    "                show_examples=False, \n",
    "                show_image=None, \n",
    "                max_corners_oob=0):\n",
    "    \"\"\"\n",
    "    This function reads images in from the file system and creates a dataset.\n",
    "\n",
    "    :param max_samples: Determines the number of sample images to load.\n",
    "    :param cards_per_image: How many cards should be in each sample image? Defaults to 1\n",
    "    :param yellow_borders_only: Determines whether the data set should be limited to trading cards with a yellow border\n",
    "    :param digital_images:\n",
    "    :param show_examples:\n",
    "    :param show_image:\n",
    "    :param max_corners_oob:\n",
    "    \"\"\"\n",
    "    my_path = PATH_TO_IMAGE_FILES\n",
    "    onlyfiles = [f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "    onlyfiles = sorted(onlyfiles)\n",
    "    os.chdir(my_path)   # cd into the directory 'my_path'\n",
    "\n",
    "    label_filename = CLASSIFICATION_RESULTS_FILENAME\n",
    "    label_file = open(label_filename, \"a\")\n",
    "    label_file.close()\n",
    "\n",
    "    x = np.array([], dtype=int)   # Numpy array holding each example image\n",
    "    y = np.array([], dtype=int)   # Array holding the labels  (only supports exactly one card/img)\n",
    "\n",
    "    i = 0\n",
    "    for filename in onlyfiles:\n",
    "        if not re.match(r\".*.jpe\", filename):\n",
    "            continue  # We only want to look at image files, skip this one if not\n",
    "\n",
    "        # Other criteria for the images:\n",
    "        file_number = int(filename[0:6])  # First 6 chars of string are image number\n",
    "        if yellow_borders_only:\n",
    "            if (file_number >= 3000) and (file_number < 9000):   \n",
    "                # Images 3000-3999 contain silver border, 4000-8999 black border, and 9000+ are digital\n",
    "                # So for yellow borders only, exclude 3000-8999\n",
    "                continue \n",
    "        if not digital_images:\n",
    "            if (file_number >= 9000):\n",
    "                # Similarly, if you don't want digital images, skip anything >9000\n",
    "                continue    \n",
    "\n",
    "        # Search the text file for the labels corresponding to this image\n",
    "        found = False\n",
    "        with open(label_filename) as file:\n",
    "            for line in file:\n",
    "                if filename in line:\n",
    "                    num_cards = get_num_cards(line)\n",
    "                    points = line_to_points(line)\n",
    "                    found = True\n",
    "                    break\n",
    "        if found and (num_cards == cards_per_image):\n",
    "            # Read in next photo\n",
    "            img = cv2.imread(filename)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "            img, points, points_oob = expand_image(img, points)\n",
    "            if (points_oob > max_corners_oob):\n",
    "                continue  # If too many corners are out-of-bounds, skip this one\n",
    "            x = np.concatenate((x, img[None,:,:,:]), axis=0) if x.size else img[None,:,:,:]\n",
    "            y = np.concatenate((y, np.array(points)[None,:,:,:]), axis=0) if y.size else np.array(points)[None,:,:,:]\n",
    "            if (show_examples and (i < 2)) or ((show_image is not None) and (show_image in filename)):\n",
    "                print(\"Image \" + str(i) + \" shape: \" + str(img.shape))\n",
    "                print(\"Corner point coordinates: \\n\" + str(np.array(points)))\n",
    "                displayRGB(img, points)\n",
    "            if (y.shape[0] >= max_samples):\n",
    "                break   # Exit once you have enough samples\n",
    "            i += 1\n",
    "\n",
    "    if x.size:\n",
    "        x = np.moveaxis(x, -1, 1)   # Move the depth to the second position\n",
    "    y = np.squeeze(y, axis=1) # Temporarily remove the 1th dimension (which contains how many cards/image) so it matches coords from DSNT\n",
    "    print(\"X shape: \" + str(x.shape))    # Dimensions are: [example, depth, height, width]\n",
    "    print(\"Y shape: \" + str(y.shape))    # Dimensions are: [example, cards in image, corners, coordinate]\n",
    "\n",
    "    os.chdir('../..')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def shuffle_two_arrays(x, y):\n",
    "    rand_idxs = np.random.permutation(y.shape[0])\n",
    "    return x[rand_idxs], y[rand_idxs]\n",
    "  \n",
    "\n",
    "def split_dataset(x, y):\n",
    "    \"\"\"\n",
    "    Split into training, validation, and test datasets\n",
    "\n",
    "    :returns: tuple of (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "    \"\"\"\n",
    "    x, y = shuffle_two_arrays(x, y)\n",
    "    num_examples = x.shape[0]\n",
    "    p8 = round(num_examples * 0.8)\n",
    "    p9 = round(num_examples * 0.9)\n",
    "    x_train = x[:p8, :, :, :]\n",
    "    try:\n",
    "      y_train = y[:p8, :, :, :]\n",
    "    except:   # If y only has 3 dimensions (when only 1 point, not 4)\n",
    "      y_train = y[:p8, :, :]\n",
    "    x_val = x[p8:p9, :, :, :]\n",
    "    try:\n",
    "      y_val = y[p8:p9, :, :, :]\n",
    "    except:\n",
    "      y_val = y[p8:p9, :, :]\n",
    "    x_test = x[p9:, :, :, :]\n",
    "    try:\n",
    "      y_test = y[p9:, :, :, :]\n",
    "    except:\n",
    "      y_test = y[p9:, :, :]\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "svLKrTyI7pGs"
   },
   "outputs": [],
   "source": [
    "# Test usage of read_images:\n",
    "# x, y = read_images(max_samples=3, show_examples=True, yellow_borders_only=True, digital_images=False, cards_per_image=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v85-WLB_X1MB",
    "outputId": "1075a4d0-50db-48b3-81e1-5878260c5a70"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/CS_6955_Project/Examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26196/1269818138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# Shown below is an example usage, which will print out an image specified by img_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# 4 times, for each rotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcards_per_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrotate_dataset_by_90s\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mimg_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m   \u001b[1;31m# Which image you want to check rotated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26196/860520389.py\u001b[0m in \u001b[0;36mread_images\u001b[1;34m(max_samples, cards_per_image, yellow_borders_only, digital_images, show_examples, show_image, max_corners_oob)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mmy_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPATH_TO_IMAGE_FILES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0monlyfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0monlyfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monlyfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_path\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# cd into the directory 'my_path'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/CS_6955_Project/Examples'"
     ]
    }
   ],
   "source": [
    "def rot_coords(y, angle, size=225):\n",
    "  '''\n",
    "  Takes in a matrix y of shape [N, 4, 2] (four (x,y) coordinates per N)\n",
    "  and rotates them by 'angle' degrees, counterclockwise.  The parameter\n",
    "  'size' is used to find the center of rotation, aka the middle of the image.\n",
    "  '''\n",
    "  y_r = copy.deepcopy(y).astype(float)  # Copy input matrix\n",
    "\n",
    "  s = np.sin(np.deg2rad(angle))\n",
    "  c = np.cos(np.deg2rad(angle))\n",
    "  rot_matrix = np.array([[c, -s],[s, c]])   # Rotational matrix\n",
    "\n",
    "  center = 225/2    # Center of rotation (middle of picture)\n",
    "\n",
    "  y_r = y_r - center  # Move image to the origin\n",
    "  y_r = np.dot(y_r, rot_matrix) # Rotate coordinates\n",
    "  y_r = y_r + center  # Move image back to original location\n",
    "\n",
    "  return y_r.astype(int)\n",
    "\n",
    "\n",
    "def rotate_dataset_by_90s(x, y):\n",
    "  '''\n",
    "  Takes in a dataset with x of shape [N, 3, height, width] and y of shape\n",
    "  [N, 4, 2] and returns (x, y) with 4*N examples.  It rotates each of the \n",
    "  dataset images by 90, 180, and 270 degrees, along with their coordinates,\n",
    "  and return the new, larger dataset.\n",
    "  '''\n",
    "  x_90 = np.rot90(x, axes=(-2, -1))\n",
    "  x_180 = np.rot90(x_90, axes=(-2, -1))\n",
    "  x_270 = np.rot90(x_180, axes=(-2, -1))\n",
    "\n",
    "  y_90 = rot_coords(y, 90)\n",
    "  y_90 = np.roll(y_90, -1, axis=1)  # Roll is used to reorder the coordinates.  Order should always be: bottom left, top left, top right, bottom right\n",
    "\n",
    "  y_180 = rot_coords(y_90, 90)\n",
    "  y_180 = np.roll(y_180, -1, axis=1)\n",
    "\n",
    "  y_270 = rot_coords(y_180, 90)\n",
    "  y_270 = np.roll(y_270, -1, axis=1)\n",
    "\n",
    "  final_x = np.vstack((x, x_90, x_180, x_270))\n",
    "  final_y = np.vstack((y, y_90, y_180, y_270))\n",
    "  \n",
    "  return final_x, final_y\n",
    "\n",
    "\n",
    "# Shown below is an example usage, which will print out an image specified by img_num\n",
    "# 4 times, for each rotation\n",
    "x, y = read_images(max_samples=3, cards_per_image=1)\n",
    "x_new, y_new = rotate_dataset_by_90s(x,y)\n",
    "img_num = 2   # Which image you want to check rotated\n",
    "for img in np.linspace(0, x_new.shape[0], num=4, endpoint=False, dtype=int): # range(x_new.shape[0]):\n",
    "  img = img + img_num\n",
    "  displayRGB(x_new[img].transpose((1,2,0)), y_new[None, img, :, :])\n",
    "  print(y_new[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ2ZM11t4aal"
   },
   "outputs": [],
   "source": [
    "def rand_gauss(m, s, w=56):   # w (max width) defaults to 56, or 225/4\n",
    "  m = int(m)\n",
    "  num = np.random.normal(m, s)\n",
    "  if num < m-w:\n",
    "    num = m-w\n",
    "  if num > m+w:\n",
    "    num = m+w;\n",
    "  if round(num) < 0:\n",
    "    print(\"ERROR: Rand_gauss returned a value < 0, m=\" + str(m) + \" s=\" + str(s) + \" w=\" + str(w))\n",
    "  if round(num) > 224:\n",
    "    print(\"ERROR: Rand_gauss returned a value of > 244: \" + str(num))\n",
    "  return round(num)\n",
    "\n",
    "\n",
    "# Creates black and white example images with perspective warp/noise\n",
    "# Use the mode 'test' to create simple images that are all square, otherwise they'll\n",
    "# be distorted rectangles.\n",
    "def create_examples(num_examples, size=225, show_examples=False, mode=None):\n",
    "  \"\"\"\n",
    "  This function creates a synthetic data set, which was useful in early development.\n",
    "  \"\"\"\n",
    "  filename = TEMPLATE_FILE_PATH\n",
    "  orig = cv2.imread(filename)\n",
    "  orig, points, _ = expand_image(orig, None, size=size)\n",
    "  if show_examples is True:\n",
    "    # orig = random_noise(orig, mode='s&p',amount=0.3)  # Converts each pixel to float from [0,1]\n",
    "    noisy_orig = random_noise(orig, mode='gaussian', clip=True, var=0.02)\n",
    "    noisy_orig = np.array(255*noisy_orig, dtype = 'uint8')        # Convert back to int in range [0,255]\n",
    "    plt.imshow(noisy_orig)\n",
    "    plt.show()\n",
    "    print(\"points on original image: \" + str(points))\n",
    "    print(\"size: \" + str(size))\n",
    "\n",
    "  if mode is 'test':\n",
    "    s = 0.0001\n",
    "  else:\n",
    "    s = round(size/11)   # Standard deviation of examples - 20 if size=225\n",
    "  w = round(size/4) # 56 if size=225\n",
    "  # The contour making a diamond through the image, we don't want points inside this diamond\n",
    "  cnt = np.array([[0, int(size/2)], [int(size/2), 0], [size, int(size/2)], [int(size/2), size]])\n",
    "\n",
    "  x = np.array([], dtype=int)   # Numpy array holding each example image\n",
    "  y = np.array([], dtype=int)   # Array holding the labels  (only supports exactly one card/img)\n",
    "\n",
    "  for ex in range(num_examples):\n",
    "    new_points = copy.deepcopy(points)\n",
    "    for i, card in enumerate(new_points):\n",
    "      new_points[i][0] = (rand_gauss(size/4, s, w=w), rand_gauss(3*size/4, s, w=w))  # Bottom left\n",
    "      while (cv2.pointPolygonTest(cnt, np_to_tuple(new_points[i][0]), False) > 0): # Positive if point is inside shape defined as cnt\n",
    "        new_points[i][0] = (rand_gauss(size/4, s, w=w), rand_gauss(3*size/4, s, w=w)) \n",
    "\n",
    "      new_points[i][1] = (rand_gauss(size/4, s, w=w), rand_gauss(size/4, s, w=w))  # Top left\n",
    "      while (cv2.pointPolygonTest(cnt, np_to_tuple(new_points[i][1]), False) > 0):\n",
    "        new_points[i][1] = (rand_gauss(size/4, s, w=w), rand_gauss(size/4, s, w=w))\n",
    "\n",
    "      new_points[i][2] = (rand_gauss(3*size/4, s, w=w), rand_gauss(size/4, s, w=w))  # Top right\n",
    "      while (cv2.pointPolygonTest(cnt, np_to_tuple(new_points[i][2]), False) > 0):\n",
    "        new_points[i][2] = (rand_gauss(3*size/4, s, w=w), rand_gauss(size/4, s, w=w))\n",
    "\n",
    "      new_points[i][3] = (rand_gauss(3*size/4, s, w=w), rand_gauss(3*size/4, s, w=w))  # Bottom right\n",
    "      while (cv2.pointPolygonTest(cnt, np_to_tuple(new_points[i][3]), False) > 0):\n",
    "        new_points[i][3] = (rand_gauss(3*size/4, s, w=w), rand_gauss(3*size/4, s, w=w))\n",
    "\n",
    "    pts1 = np.array(points, np.float32).squeeze()\n",
    "    pts2 = np.array(new_points, np.float32).squeeze()\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    img = cv2.warpPerspective(orig, M, (size, size))\n",
    "    img = random_noise(img, mode='gaussian', clip=True, var=0.02)\n",
    "    img = np.array(255*img, dtype = 'uint8')\n",
    "    x = np.concatenate((x, img[None,:,:,:]), axis=0) if x.size else img[None,:,:,:]\n",
    "    y = np.concatenate((y, np.array(new_points)[None,:,:,:]), axis=0) if y.size else np.array(new_points)[None,:,:,:]\n",
    "    if show_examples and ex < 5:\n",
    "      plt.subplot(1,5,ex+1), plt.imshow(img)\n",
    "\n",
    "  if show_examples:\n",
    "    plt.show()\n",
    "  if x.size:\n",
    "    x = np.moveaxis(x, -1, 1)   # Move the depth to the second position\n",
    "  y = np.squeeze(y, axis=1)\n",
    "  print(\"X shape: \" + str(x.shape))    # Dimensions are: [example, depth, height, width]\n",
    "  print(\"Y shape: \" + str(y.shape))    # Dimensions are: [example, cards in image, corners, coordinate]\n",
    "  # cv2.drawContours(orig, [cnt], 0, (255, 0, 0), 3)  # Draws contour over \"orig\" if you want to check it\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qU6A4SGpDRG"
   },
   "outputs": [],
   "source": [
    "def img_to_tensor(image):\n",
    "  \"\"\"\n",
    "  Convert an image to a tensor\n",
    "  \"\"\"\n",
    "  try:\n",
    "    img_tensor = torch.from_numpy(image).permute(2, 0, 1).float()   # Move the depth to the start\n",
    "    input_tensor = img_tensor.div(255).unsqueeze(0)   # If just one image, add another dimension at front\n",
    "  except:\n",
    "    img_tensor = torch.from_numpy(image).permute(0, 3, 1, 2).float()\n",
    "    input_tensor = img_tensor.div(255)\n",
    "  return input_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtE6sCJrat2y"
   },
   "outputs": [],
   "source": [
    "def coords_to_points(coords, image_size=(225, 225)):\n",
    "  output = (((coords.cpu() + 1) * torch.Tensor(image_size)) - 1) / 2\n",
    "  output = torch.round(output)\n",
    "  return output\n",
    "\n",
    "\n",
    "def points_to_coords(points, image_size=(225, 225)):\n",
    "  return ((points * 2 + 1) / torch.Tensor(image_size)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x_CG3P5MA2r"
   },
   "outputs": [],
   "source": [
    "### Override this method from dsntnn because it doesn't work on cuda ###\n",
    "# The error message is: \n",
    "#   RuntimeError: view size is not compatible with input tensor's size and \n",
    "#   stride (at least one dimension spans across two contiguous subspaces). Use \n",
    "#   .reshape(...) instead.\n",
    "# \n",
    "# On our example with 2 images with 2 points each: \n",
    "#   orig_size: torch.Size([2, 2, 40, 40])\n",
    "#   inp shape: torch.Size([2, 2, 40, 40])\n",
    "#   flat shape: torch.Size([4, 1600])\n",
    "\n",
    "def _flat_softmax(inp):\n",
    "    \"\"\"\n",
    "    Compute the softmax with all but the first two tensor dimensions combined.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_size = inp.size()\n",
    "    # print(\"orig_size: \" + str(orig_size))\n",
    "    # print(\"inp shape: \" + str(inp.shape))\n",
    "    # flat = inp.view(-1, reduce(mul, orig_size[2:])) # This was the original line, causing errors\n",
    "    flat = torch.reshape(inp, (orig_size[0]*orig_size[1], orig_size[2]*orig_size[3]))\n",
    "    # print(\"flat shape: \" + str(flat.shape))\n",
    "    flat = torch.nn.functional.softmax(flat, -1)\n",
    "    return flat.view(*orig_size)\n",
    "\n",
    "dsntnn.flat_softmax = _flat_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGWWUFkjnwmH"
   },
   "outputs": [],
   "source": [
    "def numpy_normalize_rgb(x, show_examples=False):\n",
    "  '''\n",
    "    Takes an input x of size (N, 3, H, W) and returns x_norm of same shape\n",
    "    Each RGB value will have the dataset mean subtracted from it, and divided\n",
    "    by the std.\n",
    "  '''\n",
    "  x_t = x.transpose((0, 2, 3, 1))\n",
    "  x_norm = (x_t - x_t.mean(axis=(0, 1, 2), keepdims=True)) / x_t.std(axis=(0, 1 ,2), keepdims=True)\n",
    "  x_norm = np.clip(x_norm, 0, 1)    # Images are now in the [-1,2.5] range, clip to [0,1]\n",
    "  x_norm = (x_norm * 255).astype('uint8')   # And convert back to [0,255] ints\n",
    "  if show_examples:\n",
    "    plt.subplot(1,2,1), plt.imshow(x_t[0])\n",
    "    plt.subplot(1,2,2), plt.imshow(x_norm[0])\n",
    "    plt.show()\n",
    "  x_norm = x_norm.transpose((0, 3, 1, 2))\n",
    "  return x_norm\n",
    "\n",
    "# x, y = read_images()\n",
    "# x, y = create_examples(10, size=225)\n",
    "# numpy_normalize_rgb(x, show_examples=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45D3nzOZiHbD"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, max_dist=2): \n",
    "  \"\"\"\n",
    "  Compute the accuracy of our classifications. \n",
    "  We do this by determining how many points were within a certain distance from where they should have been.\n",
    "  We will run this function several times and report the results at various thresholds.\n",
    "  :param loader:\n",
    "  :param model: Our trained model\n",
    "  :param max_dist: Our threshold distance from the target points that we will accept\n",
    "  :returns: Accurracy metric\n",
    "  \"\"\"\n",
    "  num_correct = 0\n",
    "  num_samples = 0\n",
    "  # model.eval()  # FIXME: Eval mode is broken? Gives horrible results\n",
    "  model.train()\n",
    "  with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "      x = x.to(device=device)  # move to device, e.g. GPU\n",
    "      y = y.to(device=device)\n",
    "      coords, heatmaps, _ = model(x)\n",
    "      diff = coords_to_points(coords) - coords_to_points(y)\n",
    "      diff = np.array(torch.abs(diff))\n",
    "      # Commented out is a for-loop implementation\n",
    "      # for sample in diff:\n",
    "      #   num_samples += 1\n",
    "      #   correct = 1\n",
    "      #   for point in sample:\n",
    "      #     if (point[0] > max_dist) or (point[1] > max_dist):\n",
    "      #       correct = 0\n",
    "      #   num_correct += correct\n",
    "\n",
    "      # Below is the numpy vectorized version.  You're just checking diff <= max_dist \n",
    "      # for each example, really. This means each of the 4 corners is predicted \n",
    "      # within max_dist pixels.  (i.e. a prediction of (1,1) when it should be \n",
    "      # (0,0) is still marked as correct if max_dist=1)\n",
    "      diff = np.all(np.all(diff<=max_dist, axis=2),axis=1)\n",
    "      num_correct += np.sum(diff)\n",
    "      num_samples += diff.shape[0]\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "  return acc\n",
    "\n",
    "\n",
    "def show_worst(loader, model, max_dist=2, show_up_to=5): \n",
    "  \"\"\"\n",
    "  Display the images which the largest error in classification\n",
    "  \"\"\"\n",
    "  num_correct = 0\n",
    "  num_incorrect = 0\n",
    "  num_samples = 0\n",
    "  num_shown = 0;\n",
    "  # model.eval()  # FIXME: Eval mode is broken? Gives horrible results\n",
    "  model.train()\n",
    "  with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "      x = x.to(device=device)  # move to device, e.g. GPU\n",
    "      y = y.to(device=device)\n",
    "      coords, heatmaps, _ = model(x)\n",
    "      diff = coords_to_points(coords) - coords_to_points(y)\n",
    "      diff = np.array(torch.abs(diff))\n",
    "      # Commented out is a for-loop implementation\n",
    "      for i, sample in enumerate(diff):\n",
    "        num_samples += 1\n",
    "        correct = 1\n",
    "        incorrect = 0\n",
    "        shown = False\n",
    "        for point in sample:\n",
    "          if (point[0] > max_dist) or (point[1] > max_dist):\n",
    "            correct = 0\n",
    "            incorrect = 1\n",
    "            if (num_shown < show_up_to) and shown is False:\n",
    "              num_shown += 1\n",
    "              shown = True\n",
    "              # img = x[i, :, :, :].detach().cpu().numpy()\n",
    "              # img = np.transpose(img, (1, 2, 0))\n",
    "              # plt.figure(figsize=(50,100))\n",
    "              # plt.subplot(1,show_up_to,num_shown, ), plt.imshow(img)\n",
    "        if shown is True:\n",
    "          img = x[i, :, :, :].detach().cpu().numpy()\n",
    "          img = np.transpose(img, (1, 2, 0))\n",
    "          sample = coords_to_points(coords[i])\n",
    "          points = []\n",
    "          points.append([])\n",
    "          points[0].append((sample[0][0], sample[0][1]))\n",
    "          points[0].append((sample[1, 0], sample[1, 1]))\n",
    "          points[0].append((sample[2][0], sample[2][1]))\n",
    "          points[0].append((sample[3][0], sample[3][1]))\n",
    "          # print(sample[None, :, :].tolist())\n",
    "          displayRGB(img, points)\n",
    "        num_correct += correct\n",
    "        num_incorrect += incorrect\n",
    "\n",
    "      # Below is the numpy vectorized version.  You're just checking diff <= max_dist \n",
    "      # for each example, really. This means each of the 4 corners is predicted \n",
    "      # within max_dist pixels.  (i.e. a prediction of (1,1) when it should be \n",
    "      # (0,0) is still marked as correct if max_dist=1)\n",
    "      # diff = np.all(np.all(diff<=max_dist, axis=2),axis=1)\n",
    "      # num_correct += np.sum(diff)\n",
    "      # num_samples += diff.shape[0]\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    plt.show()\n",
    "  # return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksAAqjIaiZ2l"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lrd, epoch, schedule):\n",
    "  \"\"\"\n",
    "  Code borrowed from a homework.\n",
    "  Multiply lrd to the learning rate if epoch is in schedule\n",
    "  \n",
    "  Inputs:\n",
    "  - optimizer: An Optimizer object we will use to train the model\n",
    "  - lrd: learning rate decay; a factor multiplied at scheduled epochs\n",
    "  - epochs: the current epoch number\n",
    "  - schedule: the list of epochs that requires learning rate update\n",
    "  \n",
    "  Returns: Nothing, but learning rate might be updated\n",
    "  \"\"\"\n",
    "  if epoch in schedule:\n",
    "    for param_group in optimizer.param_groups:\n",
    "      print('lr decay from {} to {}'.format(param_group['lr'], param_group['lr'] * lrd))\n",
    "      param_group['lr'] *= lrd\n",
    "\n",
    "def train(model, optimizer, epochs=1, learning_rate_decay=.1, schedule=[], verbose=True):\n",
    "  \"\"\"\n",
    "  Code borrowed from a homework.\n",
    "  Train a model using the PyTorch Module API.\n",
    "  \n",
    "  Inputs:\n",
    "  - model: A PyTorch Module giving the model to train.\n",
    "  - optimizer: An Optimizer object we will use to train the model\n",
    "  - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "  \n",
    "  Returns: Nothing, but prints model accuracies during training.\n",
    "  \"\"\"\n",
    "  model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "  num_iters = epochs * len(loader_train)\n",
    "  if verbose:\n",
    "    num_prints = num_iters // print_every + 1\n",
    "  else:\n",
    "    num_prints = epochs\n",
    "  # loss_history = torch.zeros(num_prints, dtype=torch.float)\n",
    "  loss_history = []\n",
    "  acc_history = torch.zeros(num_prints, dtype=torch.float)\n",
    "  iter_history = torch.zeros(num_prints, dtype=torch.long)\n",
    "  for e in range(epochs):\n",
    "    \n",
    "    adjust_learning_rate(optimizer, learning_rate_decay, e, schedule)\n",
    "    \n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "      model.train()  # put model to training mode\n",
    "      x = x.to(device=device)  # move to device, e.g. GPU\n",
    "      y = y.to(device=device)\n",
    "\n",
    "      # Run it through the model\n",
    "      coords, heatmaps, _ = model(x)\n",
    "\n",
    "      # Calculate the loss\n",
    "      euc_losses = dsntnn.euclidean_losses(coords, y)\n",
    "      reg_losses = dsntnn.js_reg_losses(heatmaps, y, sigma_t=1.0)\n",
    "      loss = dsntnn.average_loss(euc_losses + reg_losses)\n",
    "\n",
    "      # Zero out all of the gradients for the variables which the optimizer\n",
    "      # will update.\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # This is the backwards pass: compute the gradient of the loss with\n",
    "      # respect to each  parameter of the model.\n",
    "      loss.backward()\n",
    "\n",
    "      # Actually update the parameters of the model using the gradients\n",
    "      # computed by the backwards pass.\n",
    "      optimizer.step()\n",
    "\n",
    "      tt = t + e * len(loader_train)\n",
    "\n",
    "      if verbose and (tt % print_every == 0 or (e == epochs-1 and t == len(loader_train)-1)):\n",
    "        print('Epoch %d, Iteration %d, loss = %.4f' % (e, tt, loss.item()))\n",
    "        acc = check_accuracy(loader_val, model)\n",
    "        acc_history[tt // print_every] = acc\n",
    "        iter_history[tt // print_every] = tt\n",
    "        # loss_history[t // print_every] = loss\n",
    "        loss_history.append(loss.item())\n",
    "      elif not verbose and (t == len(loader_train)-1):\n",
    "        print('Epoch %d, Iteration %d, loss = %.4f' % (e, tt, loss.item()))\n",
    "        acc = check_accuracy(loader_val, model)\n",
    "        acc_history[e] = acc\n",
    "        iter_history[e] = tt\n",
    "        # loss_history[e] = loss\n",
    "        loss_history.append(loss.item())\n",
    "  return loss_history, acc_history, iter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFckUyhwulVZ"
   },
   "source": [
    "**Define the core components we need for our model architecture**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeNgNGcQnyu9"
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a network with the following architechture:\n",
    "    - 2D Convolutional Layer\n",
    "    - ReLU Activation\n",
    "    - 2D Batch Normalization\n",
    "    - 2D Convolutional Layer\n",
    "    - ReLU Activation\n",
    "    - 2D Batch Normalization\n",
    "    - 2D Convolutional Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, self.H, kernel_size=11, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(H),\n",
    "            nn.Conv2d(self.H, self.H, kernel_size=13, padding=6),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(self.H),\n",
    "            nn.Conv2d(self.H, self.H, kernel_size=15, padding=5, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqzmHuDbnzIU"
   },
   "outputs": [],
   "source": [
    "class CRN(nn.Module):\n",
    "    def __init__(self, n_locations, H=16):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.fcn = FCN(H)\n",
    "        self.hm_conv = nn.Conv2d(H, n_locations, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 1. Run the images through our FCN\n",
    "        fcn_out = self.fcn(images)    # Shape [N, H, 255, 255]\n",
    "        # 2. Use a 1x1 conv to get one unnormalized heatmap per location\n",
    "        unnormalized_heatmaps = self.hm_conv(fcn_out)\n",
    "        # print(\"u heatmaps: \" + str(unnormalized_heatmaps.shape))\n",
    "        # 3. Normalize the heatmaps\n",
    "        heatmaps = dsntnn.flat_softmax(unnormalized_heatmaps)\n",
    "        # print(\"heatmaps: \" + str(heatmaps.shape))\n",
    "        # 4. Calculate the coordinates\n",
    "        coords = dsntnn.dsnt(heatmaps)\n",
    "\n",
    "        return coords, heatmaps, unnormalized_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7GUHxY3XDHv"
   },
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  Code borrowed from our ResNet homework.\n",
    "  \"\"\"\n",
    "  def __init__(self, Cin, Cout, downsample=False, kernel=3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement plain block.                                             #\n",
    "    # Hint: Wrap your layers by nn.Sequential() to output a single module.     #\n",
    "    #       You don't have use OrderedDict.                                    #\n",
    "    # Inputs:                                                                  #\n",
    "    # - Cin: number of input channels                                          #\n",
    "    # - Cout: number of output channels                                        #\n",
    "    # - downsample: add downsampling (a conv with stride=2) if True            #\n",
    "    # Store the result in self.net.                                            #\n",
    "    ############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    strd = 1\n",
    "    if downsample:\n",
    "      strd = 2\n",
    "\n",
    "    num_hidden_channels = int((Cin + Cout) / 2)\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.BatchNorm2d(Cin), # The doc says num_features is C from expected input of size (N, C, H, W)\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(Cin, num_hidden_channels, kernel, padding=int((kernel-1)/2), stride=strd),\n",
    "        nn.BatchNorm2d(num_hidden_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(num_hidden_channels, Cout, kernel, padding=int((kernel-1)/2))\n",
    "    )\n",
    "    ############################################################################\n",
    "    #                                 END OF YOUR CODE                         #\n",
    "    ############################################################################\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTrIl2UkW8u3"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  Code borrowed from our ResNet homework.\n",
    "  \"\"\"\n",
    "  def __init__(self, Cin, Cout, downsample=False, kernel=3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.block = None # F\n",
    "    self.shortcut = None # G\n",
    "    ############################################################################\n",
    "    # TODO: Implement residual block using plain block. Hint: nn.Identity()    #\n",
    "    # Inputs:                                                                  #\n",
    "    # - Cin: number of input channels                                          #\n",
    "    # - Cout: number of output channels                                        #\n",
    "    # - downsample: add downsampling (a conv with stride=2) if True            #\n",
    "    # Store the main block in self.block and the shortcut in self.shortcut.    #\n",
    "    ############################################################################\n",
    "    strd = 1\n",
    "    if downsample:\n",
    "      strd = 2\n",
    "\n",
    "    self.block = PlainBlock(Cin, Cout, downsample=downsample, kernel=kernel)\n",
    "    self.shortcut = nn.Sequential(\n",
    "        nn.Conv2d(Cin, Cout, 1, stride=strd)\n",
    "    )\n",
    "    ############################################################################\n",
    "    #                                 END OF YOUR CODE                         #\n",
    "    ############################################################################\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.block(x) + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjfvR0cQW2wI"
   },
   "outputs": [],
   "source": [
    "class ResNetStage(nn.Module):\n",
    "  \"\"\"\n",
    "  Code borrowed from our ResNet homework.\n",
    "  \"\"\"\n",
    "  def __init__(self, Cin, Cout, num_blocks, downsample=True,\n",
    "               block=ResidualBlock, kernel=3):\n",
    "    super().__init__()\n",
    "    blocks = [block(Cin, Cout, downsample, kernel=kernel)]\n",
    "    for _ in range(num_blocks - 1):\n",
    "      blocks.append(block(Cout, Cout, kernel=kernel))\n",
    "    self.net = nn.Sequential(*blocks)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQx8e_BFW0Qh"
   },
   "outputs": [],
   "source": [
    "class ResNetStem(nn.Module):\n",
    "  \"\"\"\n",
    "  Code borrowed from our ResNet homework.\n",
    "  \"\"\"\n",
    "  def __init__(self, Cin=3, Cout=8, kernel=3):\n",
    "    super().__init__()\n",
    "    layers = [\n",
    "        nn.Conv2d(Cin, Cout, kernel_size=kernel, padding=1, stride=1),\n",
    "        nn.ReLU(),\n",
    "    ]\n",
    "    self.net = nn.Sequential(*layers)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQqv8uzqWhYK"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Code borrowed from our ResNet homework.\n",
    "  \"\"\"\n",
    "  def __init__(self, stage_args, Cin=3, block=ResidualBlock, kernel=None, n_locations=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cnn = None\n",
    "    # stage_args is Cin, Cout, Num Blocks, Downsample\n",
    "    if kernel is None:\n",
    "      kernel = [3, 3, 3, 3, 3, 3]\n",
    "    stages = [ResNetStem(Cin, stage_args[0][0], kernel=kernel[0])]  # Interface with first actual stage - get right amount of channels\n",
    "    for i, stage in enumerate(stage_args):\n",
    "      stages.append(ResNetStage(stage[0], stage[1], stage[2], kernel=kernel[i], downsample=stage[3], block=block))\n",
    "\n",
    "    self.cnn = nn.Sequential(*stages)\n",
    "    ############################################################################\n",
    "    #                                 END OF YOUR CODE                         #\n",
    "    ############################################################################\n",
    "    self.hm_conv = nn.Conv2d(stage_args[-1][1], n_locations, kernel_size=1, bias=False)\n",
    "  \n",
    "  def forward(self, images):\n",
    "      # 1. Run the images through our FCN\n",
    "      fcn_out = self.cnn(images)    # Shape [N, H, 255, 255]\n",
    "\n",
    "      # 2. Use a 1x1 conv to get one unnormalized heatmap per location\n",
    "      unnormalized_heatmaps = self.hm_conv(fcn_out) # [N, n_locations, 255, 255]\n",
    "\n",
    "      # 3. Normalize the heatmaps\n",
    "      heatmaps = dsntnn.flat_softmax(unnormalized_heatmaps)\n",
    "\n",
    "      # 4. Calculate the coordinates\n",
    "      coords = dsntnn.dsnt(heatmaps)\n",
    "\n",
    "      return coords, heatmaps, unnormalized_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6u3jdaGkgO_"
   },
   "outputs": [],
   "source": [
    "# Dictionary of networks that are premade to pull from: \n",
    "networks = {\n",
    "  'plain4': {\n",
    "    'block': PlainBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False),               \n",
    "    ]    \n",
    "  },\n",
    "  'plain4k': {\n",
    "    'block': PlainBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False),               \n",
    "    ],\n",
    "    'kernel': [15, 17]\n",
    "  },\n",
    "  'plain6': {             # Final loss: 0.00823.  Achieve 40/40 at 5 pixels away,\n",
    "    'block': PlainBlock,  # 37/40 (92.5%) at 4, with lr = 2.5e-3       \n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False),\n",
    "      (16, 32, 1, False)             \n",
    "    ]\n",
    "  },\n",
    "  'plain6k': {         \n",
    "    'block': PlainBlock,      \n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False),\n",
    "      (16, 32, 1, False)             \n",
    "    ],\n",
    "    'kernel': [9, 13, 19]\n",
    "  },\n",
    "  'plain32': {\n",
    "    'block': PlainBlock,\n",
    "    'stage_args': [\n",
    "      (8, 8, 5, False),\n",
    "      (8, 16, 5, False),\n",
    "      (16, 32, 5, False),\n",
    "    ]\n",
    "  },\n",
    "  'resnet4': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False)             \n",
    "    ]\n",
    "  },\n",
    "  'resnet4k': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False)             \n",
    "    ],\n",
    "    'kernel': [15, 17]\n",
    "  },\n",
    "  'resnet4kD': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 1, True)             \n",
    "    ],\n",
    "    'kernel': [15, 17]\n",
    "  },\n",
    "  'resnet6k': { \n",
    "    'block': ResidualBlock,      \n",
    "    'stage_args': [\n",
    "      (8, 16, 1, False),\n",
    "      (16, 32, 1, False)             \n",
    "    ],\n",
    "    'kernel': [15, 17, 19]\n",
    "  },\n",
    "  'resnet6kD': { \n",
    "    'block': ResidualBlock,      \n",
    "    'stage_args': [\n",
    "      (8, 16, 1, True),\n",
    "      (16, 32, 1, False)             \n",
    "    ],\n",
    "    'kernel': [15, 17, 19]\n",
    "  },\n",
    "  'resnet8': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 2, False),\n",
    "      (16, 32, 1, False)               \n",
    "    ]\n",
    "  },\n",
    "  'resnet8k': { \n",
    "    'block': ResidualBlock,      \n",
    "    'stage_args': [\n",
    "      (8, 16, 2, False),\n",
    "      (16, 32, 1, False)             \n",
    "    ],\n",
    "    'kernel': [15, 17, 19]\n",
    "  },\n",
    "  'resnet12': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 16, 3, False),\n",
    "      (16, 32, 2, False)               \n",
    "    ]\n",
    "  },\n",
    "  'resnet32': {\n",
    "    'block': ResidualBlock,\n",
    "    'stage_args': [\n",
    "      (8, 8, 5, False),\n",
    "      (8, 16, 5, False),\n",
    "      (16, 32, 5, False),\n",
    "    ]\n",
    "  },\n",
    "}\n",
    "\n",
    "def get_net(name):\n",
    "  if name is 'crn':\n",
    "    return CRN(n_locations=4, H=16)\n",
    "  else:\n",
    "    return ResNet(**networks[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPDm0e70djYm"
   },
   "outputs": [],
   "source": [
    "# print(get_net('resnet8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLpJ7_2IuvkX"
   },
   "source": [
    "**Set up for Training**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REKVQIcXequa",
    "outputId": "f902d843-ff47-43c2-e27f-fca27105db85"
   },
   "outputs": [],
   "source": [
    "# x, y = create_examples(400, show_examples=False, size=225) # if mode=='test', it will generate only simple squares\n",
    "x, y = read_images(max_samples=8000, show_examples=False, yellow_borders_only=False)\n",
    "x, y = rotate_dataset_by_90s(x,y)\n",
    "print(\"X shape after transformations: \" + str(x.shape))\n",
    "\n",
    "# x = numpy_normalize_rgb(x)\n",
    "\n",
    "# Split the data set\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_dataset(x, y)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "NUM_TRAIN = x_train.shape[0]\n",
    "NUM_VAL = x_val.shape[0]\n",
    "NUM_TEST = x_val.shape[0]\n",
    "\n",
    "x_train_var = torch.from_numpy(x_train).div(255)\n",
    "y_train_var = torch.from_numpy(y_train)\n",
    "y_train_var = points_to_coords(y_train_var, image_size=(225, 225))\n",
    "x_val_var = torch.from_numpy(x_val).div(255)\n",
    "y_val_var = torch.from_numpy(y_val)\n",
    "y_val_var = points_to_coords(y_val_var, image_size=(225, 225))\n",
    "x_test_var = torch.from_numpy(x_test).div(255)\n",
    "y_test_var = torch.from_numpy(y_test)\n",
    "y_test_var = points_to_coords(y_test_var, image_size=(225, 225))\n",
    "\n",
    "# Get rid of the numpy arrays now that there are copies as tensors\n",
    "del x_train\n",
    "del y_train\n",
    "del x_val\n",
    "del y_val\n",
    "del x_test\n",
    "del y_test\n",
    "\n",
    "train_dataset = TensorDataset(x_train_var, y_train_var)\n",
    "loader_train = DataLoader(train_dataset, batch_size=16, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "print(str(len(loader_train)) + \" minibatches in the training dataset\")\n",
    "\n",
    "val_dataset = TensorDataset(x_val_var, y_val_var)\n",
    "loader_val = DataLoader(val_dataset, batch_size=16, sampler=sampler.SubsetRandomSampler(range(NUM_VAL)))\n",
    "\n",
    "test_dataset = TensorDataset(x_test_var, y_test_var)\n",
    "loader_test = DataLoader(test_dataset, batch_size=16, sampler=sampler.SubsetRandomSampler(range(NUM_TEST)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYLb0oU0u6Xo"
   },
   "source": [
    "**Perform Training**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2-p2NXR3knJK",
    "outputId": "9c6e831d-332b-4c1c-9190-dac7f33b7612"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train our model\n",
    "#\n",
    "\n",
    "# model = get_net('crn')\n",
    "model = get_net('resnet6kD')\n",
    "\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=2.5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2.5e-3)\n",
    "\n",
    "loss_history, acc_history, iter_history = train(model, optimizer, epochs=40, learning_rate_decay=0.5, schedule=[25, 30, 35], verbose=False)\n",
    "\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.show()\n",
    "print(\"final loss: \" + str(loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9x86qxWjvSUs",
    "outputId": "76b3ee71-d3be-409f-f12a-f639a963c09e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Check the accuracy\n",
    "#\n",
    "\n",
    "acc = check_accuracy(loader_val, model, max_dist=5)\n",
    "acc = check_accuracy(loader_val, model, max_dist=4)\n",
    "acc = check_accuracy(loader_val, model, max_dist=3)\n",
    "acc = check_accuracy(loader_val, model, max_dist=2)\n",
    "acc = check_accuracy(loader_val, model, max_dist=1)\n",
    "acc = check_accuracy(loader_val, model, max_dist=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATlHu8vDOpyM"
   },
   "source": [
    "**Show the Network Architecture**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyLWkudAOwJ2",
    "outputId": "abe5f631-e0e1-4f0d-a689-0b6ec3c57b9f"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyO1oewXu_MV"
   },
   "source": [
    "**Visualize the Results**\n",
    "\n",
    "---\n",
    "\n",
    "We will show heat maps from the network and then display the images which had the greatest classification errors so we can learn what type of problems our network is having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "CTE6Xu9ymxtf",
    "outputId": "df88112d-e921-4eed-a639-6bc6c4966f47"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Visualize heatmaps from the network\n",
    "#\n",
    "coords, _, heatmaps = model(x_train_var[0:1].to(device))\n",
    "\n",
    "print(coords_to_points(coords[0,0]))\n",
    "print(coords_to_points(coords[0,1]))\n",
    "print(coords_to_points(coords[0,2]))\n",
    "print(coords_to_points(coords[0,3]))\n",
    "print(coords_to_points(y_train_var[0]))\n",
    "\n",
    "plt.subplot(1,4,1), plt.imshow(heatmaps[0, 0].detach().cpu().numpy())\n",
    "plt.subplot(1,4,2), plt.imshow(heatmaps[0, 1].detach().cpu().numpy())\n",
    "plt.subplot(1,4,3), plt.imshow(heatmaps[0, 2].detach().cpu().numpy())\n",
    "plt.subplot(1,4,4), plt.imshow(heatmaps[0, 3].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "npL5IFf_4SwQ",
    "outputId": "b0ba2b0a-76d7-4689-d09b-160e884161a0"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Show the images which had the highest error\n",
    "#\n",
    "show_worst(loader_val, model, max_dist=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Corner Detection - Final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
