{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvib9TpmHvP-"
   },
   "source": [
    "# Notebook Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "3PgaOsSe9UAv"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "### Note: this stuff is also borrowed from Corner_detection.ipynb ###\n",
    "\n",
    "#\n",
    "# Import required libraries and helper functions\n",
    "#\n",
    "\n",
    "import os     # Used to sort files for file reading\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re     # Regex, used in the file reading\n",
    "\n",
    "import cv2          # Open Computer Vision 2 - a must for any image manipulation\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np  # Other staples for working with images\n",
    "import random\n",
    "import copy\n",
    "from skimage.util import random_noise\n",
    "\n",
    "import torch\n",
    "from torch import nn    # Pytorch\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, sampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import scipy.misc       # Don't remember what this is used for - may not be needed\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wiSQwMlDe_y",
    "outputId": "90988d4c-5c1c-4341-fbe0-c3df791b3680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CS_6955_Project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Clone our labeled images stored in GitHub\n",
    "\n",
    "!git clone https://github.com/Hunterdjensen/CS_6955_Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoAM_jAiDjE-",
    "outputId": "2316c159-d4f5-4649-f483-40f0e31e912b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Determine what type of device we are using\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIl3cv7eHENi"
   },
   "source": [
    "# General Helper Functions\n",
    "Borrowed from Corner_detection.ipynb, with a few minor edits\n",
    "to match the segmentation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6UYc52kPDput"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helper functions for parsing the images\n",
    "#\n",
    "\n",
    "def line_to_points(line):\n",
    "    points_ascii = line.split(\"\\t\")[2]  # The 3rd column contains points data\n",
    "    points = eval(points_ascii)         # Convert the string into a list\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_num_cards(line):\n",
    "    return int(line.split(\"\\t\")[1])\n",
    "\n",
    "\n",
    "# Function for displaying BGR images (opencv defaults to reading images in BGR format)\n",
    "def displayRGB(image, points=None):\n",
    "    #Compatibility with (C, W, H) data\n",
    "    if image.shape[0] == 3:\n",
    "        image = np.moveaxis(image, 0, -1)\n",
    "    temp_img = image.copy()\n",
    "    if points is not None:\n",
    "        for j, frame in enumerate(points):\n",
    "            for i, point in enumerate(frame):\n",
    "                cv2.line(temp_img, tuple(point), tuple(points[j][i - 1]), (255, 0, 0), thickness=2)\n",
    "    plt.axis(\"off\")\n",
    "    # plt.imshow(cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(temp_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# So that we can take various sized rectangular photos and make them all square:\n",
    "def expand_image(img, points, size=225):\n",
    "    height, width, _ = img.shape\n",
    "    points_oob = 0\n",
    "\n",
    "    # Check how many points are out of bounds\n",
    "    if points is not None:\n",
    "      for card in points:\n",
    "        for point in card:\n",
    "          if (point[0] < 0) or (point[0] >= width) or (point[1] < 0) or (point[1] >= height):\n",
    "            points_oob += 1\n",
    "    \n",
    "    # If picture is too big, scaled it down to the size\n",
    "    if (height > width):\n",
    "        if (height > size):\n",
    "            img = imutils.resize(img, height=size)\n",
    "    else:\n",
    "        if (width > size):\n",
    "            img = imutils.resize(img, width=size)\n",
    "\n",
    "    # Regrab the new height and width\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # If no points were passed in, assume the corners of the image\n",
    "    if points is None:\n",
    "        points = []\n",
    "        points.append([])\n",
    "        points[0].append((0, height))\n",
    "        points[0].append((0, 0))\n",
    "        points[0].append((width, 0))\n",
    "        points[0].append((width, height))\n",
    "\n",
    "\n",
    "    # Add border (if the image isn't already square)\n",
    "    left = int((size - width) / 2)  # So if image has width of 125, then left is 100/2=50\n",
    "    right = int(size - (left+width))  # Remainder\n",
    "    top = int((size - height) / 2)\n",
    "    bottom = int(size - (top+height))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\n",
    "    # Update points\n",
    "    for i, card in enumerate(points):      # The first dim of 'points' is each card\n",
    "        for j, point in enumerate(card):   # Second dimension is a list of 4 tuples (each corner point)\n",
    "            points[i][j] = (point[0]+left, point[1]+top)    # Shift it to match the new image\n",
    "\n",
    "    return (img, points, points_oob)\n",
    "\n",
    "\n",
    "def read_images(max_samples=50000, cards_per_image=1, show_examples=False, show_image=None, max_corners_oob=0):\n",
    "    my_path = 'CS_6955_Project/Examples'\n",
    "    onlyfiles = [f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "    onlyfiles = sorted(onlyfiles)\n",
    "    os.chdir(my_path)   # cd into the directory 'my_path'\n",
    "\n",
    "    label_filename = 'classification_results.txt'\n",
    "    label_file = open(label_filename, \"a\")\n",
    "    label_file.close()\n",
    "\n",
    "    x = np.array([], dtype=int)   # Numpy array holding each example image\n",
    "    y = np.array([], dtype=int)   # Array holding the labels  (only supports exactly one card/img)\n",
    "\n",
    "    i = 0\n",
    "    for filename in onlyfiles:\n",
    "        # Search the text file for the labels corresponding to this image\n",
    "        found = False\n",
    "        with open(label_filename) as file:\n",
    "            for line in file:\n",
    "                if filename in line:\n",
    "                    num_cards = get_num_cards(line)\n",
    "                    points = line_to_points(line)\n",
    "                    found = True\n",
    "                    break\n",
    "        if found and re.match(r\".*.jpe\", filename) and (num_cards == cards_per_image):\n",
    "            # Read in next photo\n",
    "            img = cv2.imread(filename)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img, points, points_oob = expand_image(img, points)\n",
    "            if (points_oob > max_corners_oob):\n",
    "                continue  # If too many corners are out-of-bounds, skip this one\n",
    "            x = np.concatenate((x, img[None,:,:,:]), axis=0) if x.size else img[None,:,:,:]\n",
    "            y = np.concatenate((y, np.array(points)[None,:,:,:]), axis=0) if y.size else np.array(points)[None,:,:,:]\n",
    "            if (show_examples and (i < 2)) or ((show_image is not None) and (show_image in filename)):\n",
    "                displayRGB(img, points)\n",
    "            if (y.shape[0] >= max_samples):\n",
    "                break   # Exit once you have enough samples\n",
    "            i += 1\n",
    "\n",
    "    if x.size:\n",
    "        x = np.moveaxis(x, -1, 1)   # Move the depth to the second position\n",
    "    #y = np.squeeze(y, axis=1) # Temporarily remove the 1th dimension (which contains how many cards/image) so it matches coords from DSNT\n",
    "    print(\"X shape: \" + str(x.shape))    # Dimensions are: [example, depth, height, width]\n",
    "    print(\"Y shape: \" + str(y.shape))    # Dimensions are: [example, cards in image, corners, coordinate]\n",
    "    \n",
    "    os.chdir('../..')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def shuffle_two_arrays(x, y):\n",
    "    rand_idxs = np.random.permutation(y.shape[0])\n",
    "    return x[rand_idxs], y[rand_idxs]\n",
    "  \n",
    "\n",
    "def split_dataset(x, y):\n",
    "    x, y = shuffle_two_arrays(x, y)\n",
    "    num_examples = x.shape[0]\n",
    "    p8 = round(num_examples * 0.8)\n",
    "    p9 = round(num_examples * 0.9)\n",
    "    x_train = x[:p8, :, :, :]\n",
    "    try:\n",
    "      y_train = y[:p8, :, :, :]\n",
    "    except:   # If y only has 3 dimensions (when only 1 point, not 4)\n",
    "      y_train = y[:p8, :, :]\n",
    "    x_val = x[p8:p9, :, :, :]\n",
    "    try:\n",
    "      y_val = y[p8:p9, :, :, :]\n",
    "    except:\n",
    "      y_val = y[p8:p9, :, :]\n",
    "    x_test = x[p9:, :, :, :]\n",
    "    try:\n",
    "      y_test = y[p9:, :, :, :]\n",
    "    except:\n",
    "      y_test = y[p9:, :, :]\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sean\\\\Documents\\\\GitHub\\\\Portfolio\\\\Pokemon Card Detection'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9ECEOu8jEzM8"
   },
   "outputs": [],
   "source": [
    "# x, y = read_images(max_samples=50, show_examples=True, cards_per_image=4)\n",
    "# displayRGB(x[0, :, :, :], y[0, :, :, :])\n",
    "\n",
    "# X = (N, C=3, W=225, H=225)\n",
    "# Y = (N, num_cards, num_points=4, point_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vfjppzceYN-5"
   },
   "outputs": [],
   "source": [
    "def numpy_normalize_rgb(x, show_examples=False):\n",
    "  '''\n",
    "    Takes an input x of size (N, 3, H, W) and returns x_norm of same shape\n",
    "    Each RGB value will have the dataset mean subtracted from it, and divided\n",
    "    by the std.\n",
    "  '''\n",
    "  x_t = x.transpose((0, 2, 3, 1))\n",
    "  x_norm = (x_t - x_t.mean(axis=(0, 1, 2), keepdims=True)) / x_t.std(axis=(0, 1 ,2), keepdims=True)\n",
    "  x_norm = np.clip(x_norm, 0, 1)    # Images are now in the [-1,2.5] range, clip to [0,1]\n",
    "  x_norm = (x_norm * 255).astype('uint8')   # And convert back to [0,255] ints\n",
    "  if show_examples:\n",
    "    plt.subplot(1,2,1), plt.imshow(x_t[0])\n",
    "    plt.subplot(1,2,2), plt.imshow(x_norm[0])\n",
    "    plt.show()\n",
    "  x_norm = x_norm.transpose((0, 3, 1, 2))\n",
    "  return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_42_OHyVHcWz"
   },
   "source": [
    "# My Helper Functions\n",
    "Things I created while making the segmentation net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "luJH2jKxHjAH"
   },
   "outputs": [],
   "source": [
    "#Converts a set of points to a segmentation mask for a single image\n",
    "def points_to_segmentation(image, points, show=False):\n",
    "  # image.shape = (C=3, W=225, H=225)\n",
    "  # points.shape = (num_cards, num_points=4, point_dim=2)\n",
    "  # returns: segments = (W=225, H=225)\n",
    "  #          mask is where segments[:, :] == 1\n",
    "  #          0 otherwise\n",
    "\n",
    "  (C, W, H) = image.shape\n",
    "  image = np.moveaxis(image, 0, -1)\n",
    "  segment_image = np.zeros_like(image)\n",
    "\n",
    "  for n in range(points.shape[0]):\n",
    "    cur_points = []\n",
    "    cv2.fillConvexPoly(segment_image, points[n], (255, 0, 0))\n",
    "  \n",
    "  if(show == True):\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(segment_image, alpha=0.5)\n",
    "    plt.show()\n",
    "  \n",
    "  ret = np.zeros((W, H))\n",
    "  ret[segment_image.sum(axis=2) != 0] = 1\n",
    "  return ret\n",
    "\n",
    "#out = points_to_segmentation(x[0], y[0], show=True)\n",
    "#plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6y8RihsWZRf",
    "outputId": "91afa357-7aeb-44fd-905d-c5d4123d2892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2861, 3, 225, 225)\n",
      "Y shape: (2861, 1, 4, 2)\n",
      "X shape: (36, 3, 225, 225)\n",
      "Y shape: (36, 2, 4, 2)\n",
      "X shape: (17, 3, 225, 225)\n",
      "Y shape: (17, 3, 4, 2)\n",
      "X shape: (8, 3, 225, 225)\n",
      "Y shape: (8, 4, 4, 2)\n",
      "X shape: (4, 3, 225, 225)\n",
      "Y shape: (4, 5, 4, 2)\n",
      "X shape: (5, 3, 225, 225)\n",
      "Y shape: (5, 6, 4, 2)\n",
      "X shape: (0,)\n",
      "Y shape: (0,)\n",
      "X shape: (4, 3, 225, 225)\n",
      "Y shape: (4, 8, 4, 2)\n",
      "X shape: (12, 3, 225, 225)\n",
      "Y shape: (12, 9, 4, 2)\n",
      "X shape: (2, 3, 225, 225)\n",
      "Y shape: (2, 10, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compiles the data into train, val, test sets suitable for the segmentation net\n",
    "# returns pytorch tensors for (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "# Note:  For memory concerns, data tensors are kept on CPU\n",
    "#        Transfer batches to/from cuda when using them\n",
    "def compile_data(device=torch.device('cpu'), dtype=torch.float32):    \n",
    "  total_data = 0\n",
    "  for c in range(1, 11): # Retrieve images with different amounts of cards\n",
    "    (cur_x, cur_y) = read_images(max_samples=10000, show_examples=False, cards_per_image=c)\n",
    "    if(len(cur_x) == 0):\n",
    "      continue\n",
    "    (N, C, W, H) = cur_x.shape\n",
    "    total_data = total_data + N\n",
    "\n",
    "    # Turn the point labels into segmentation labels\n",
    "    cur_y_masks = np.zeros((N, W, H))\n",
    "    for img in range(N):\n",
    "      cur_y_masks[img, :, :] = points_to_segmentation(cur_x[img], cur_y[img])\n",
    "\n",
    "    # Add data to the collection\n",
    "    if c == 1:\n",
    "      all_np_x = cur_x\n",
    "      all_np_y = cur_y_masks\n",
    "    else:\n",
    "      all_np_x = np.concatenate((all_np_x, cur_x))\n",
    "      all_np_y = np.concatenate((all_np_y, cur_y_masks))\n",
    "    \n",
    "  assert total_data == all_np_x.shape[0]\n",
    "\n",
    "  # Process data for the net\n",
    "  np_sets = split_dataset(numpy_normalize_rgb(all_np_x), all_np_y)\n",
    "  tensors = []\n",
    "  for arr in np_sets:\n",
    "    tensors.append(torch.tensor(arr).to(device, dtype))\n",
    "\n",
    "  tensors.insert(0, all_np_x)\n",
    "  tensors.insert(1, all_np_y)\n",
    "\n",
    "  return tuple(tensors)\n",
    "\n",
    "\n",
    "# The result of this function, stored as global variables\n",
    "(all_imgs, all_labels, x_train, y_train, x_val, y_val, x_test, y_test) = compile_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTPzI3GgfGv5",
    "outputId": "1ded882c-a2f7-49a8-d854-a20375e42e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2359, 3, 225, 225])\n",
      "torch.Size([2359, 225, 225])\n",
      "torch.Size([295, 3, 225, 225])\n",
      "torch.Size([295, 225, 225])\n",
      "torch.Size([295, 3, 225, 225])\n",
      "torch.Size([295, 225, 225])\n"
     ]
    }
   ],
   "source": [
    "for m in (x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "  print(m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN3rMnz5pO5Z"
   },
   "source": [
    "# Our SegNet Structure\n",
    "This neural net is based around SegNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXXzOnPrIXlF"
   },
   "source": [
    "\n",
    "**Sources used:**\n",
    "*  SegNet paper - https://arxiv.org/pdf/1511.00561.pdf\n",
    "  * Key Ideas:\n",
    "  * Encoder portion is essentially the convolutional part of VGG16, pretrained on ImageNet\n",
    "  * Pooling is achieved by Max Sampling\n",
    "  * Blocks in the Decoder correspond to blocks in the Encoder with the same W/H, and upsampling to these blocks uses the indices of the max sampling in the corresponding encoder block\n",
    "  * The upsampling used is sparse, in that all other values are 0s.\n",
    "*  VGGNet structure - https://neurohive.io/en/popular-networks/vgg16/\n",
    "  * Used for the design of the VGG-like SegNets\n",
    "  * Their implementations are available below, but because of memory issues they aren't tested\n",
    "* The paper for an earlier version of SegNet, now termed \"SegNet_Basic\" - https://arxiv.org/pdf/1505.07293.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V_spDD0Z0c6d"
   },
   "outputs": [],
   "source": [
    "# A convolutional layer, combined with a batch norm and a ReLU\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, channels_list, kernel_size, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "    # channels_list:   A list of in/out channels, where channels_list[n] is the\n",
    "    #                  in_channel for the n'th conv layer, as well as the\n",
    "    #                  out_channel for the n-1'th conv layer.\n",
    "    #                  Channels_list[0] is the in_channels for the block,\n",
    "    #                  and channels_list[-1] is the out_channels for the block.\n",
    "    # kernel_size:     Size of the kernel in each convolutional layer.\n",
    "    super().__init__()\n",
    "    layers = []\n",
    "    for i in range(1, len(channels_list)):\n",
    "      #Layers in a VGG16 net\n",
    "      layers.append(nn.Conv2d(channels_list[i-1], channels_list[i], kernel_size=kernel_size, padding=math.floor(kernel_size/2)).to(device, dtype))\n",
    "      layers.append(nn.BatchNorm2d(channels_list[i]).to(device, dtype))\n",
    "      layers.append(nn.ReLU().to(device, dtype))\n",
    "    self.net = nn.Sequential(*layers).to(device, dtype)\n",
    "  \n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c0Ol8W6vjMat"
   },
   "outputs": [],
   "source": [
    "# A block of convolutions in the encoder\n",
    "# Ends with a max pool layer\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "  def __init__(self, channels_list, kernel_size, pooling_factor, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "    # channels_list:   A list of in/out channels, where channels_list[n] is the\n",
    "    #                  in_channel for the n'th conv layer, as well as the\n",
    "    #                  out_channel for the n-1'th conv layer.\n",
    "    #                  Channels_list[0] is the in_channels for the block,\n",
    "    #                  and channels_list[-1] is the out_channels for the block.\n",
    "    # kernel_size:     Size of the kernel in each convolutional layer.\n",
    "    # pooling_factor:  The kernel/stride of the maxpool.\n",
    "    #                  Divides the W/H of the image by pooling_factor.\n",
    "    super().__init__()\n",
    "    self.ConvNet = ConvBlock(channels_list, kernel_size, dtype=dtype, device=device).to(device, dtype)\n",
    "    #Downsampling via Maxpool\n",
    "    self.MaxPool = nn.MaxPool2d(pooling_factor, pooling_factor, return_indices=True).to(device, dtype)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    # input:  x = (N, C, W, H)\n",
    "    # output: x_out = (N, C_out, W_out, H_out)\n",
    "    #         indices = (N, C, W, H)? shouldn't need to deal with size here\n",
    "    #         (W_in, H_in)\n",
    "    x = self.ConvNet(x)\n",
    "    (N, C, W, H) = x.shape\n",
    "    (x, indices) = self.MaxPool(x)\n",
    "    # For memory concerncs, temporarily take the indices off the gpu\n",
    "    return x, indices.to(torch.device('cpu')), W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4aMDvQ0zwFlM"
   },
   "outputs": [],
   "source": [
    "# A block of convolutions in the decoder\n",
    "# Starts with an up-sampling layer (MaxUnpool)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "  def __init__(self, channels_list, kernel_size, unpooling_factor, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "    # channels_list:     A list of in/out channels, where channels_list[n] is the\n",
    "    #                    in_channel for the n'th conv layer, as well as the\n",
    "    #                    out_channel for the n-1'th conv layer.\n",
    "    #                    Channels_list[0] is the in_channels for the block,\n",
    "    #                    and channels_list[-1] is the out_channels for the block.\n",
    "    # kernel_size:       Size of the kernel in each convolutional layer.\n",
    "    # unpooling_factor:  the kernel_size/stride of the unpooling layer\n",
    "    super().__init__()\n",
    "    self.unpool = nn.MaxUnpool2d(kernel_size=unpooling_factor, stride=unpooling_factor).to(device, dtype)\n",
    "    self.ConvNet = ConvBlock(channels_list, kernel_size, dtype=dtype, device=device).to(device, dtype)\n",
    "    \n",
    "\n",
    "  def forward(self, x, unpool_params):\n",
    "    (indices, W, H) = unpool_params\n",
    "    # Put the indices back on the gpu\n",
    "    x = self.unpool(x, indices.to(device), output_size=torch.Size([x.shape[0], x.shape[1], W, H]))\n",
    "    return self.ConvNet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C9TrBpmb2jRB"
   },
   "outputs": [],
   "source": [
    "# The combined network\n",
    "\n",
    "class OurSegNet(nn.Module):\n",
    "  def __init__(self, net_params, dtype=torch.float32, device=torch.device('cuda:0')):\n",
    "    # net_params:  A dictionary of parameters for the blocks in the net\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.dtype = dtype\n",
    "    self.DownBlocks = torch.nn.ModuleList()\n",
    "    self.UpBlocks = torch.nn.ModuleList()\n",
    "\n",
    "    for n in net_params['DownBlocks']:\n",
    "      self.DownBlocks.append(DownBlock(n['channels_list'], n['kernel_size'], n['pooling_factor'], dtype=dtype, device=device))\n",
    "    for n in net_params['UpBlocks']:\n",
    "      self.UpBlocks.append(UpBlock(n['channels_list'], n['kernel_size'], n['unpooling_factor'], dtype=dtype, device=device))\n",
    "    if len(self.DownBlocks) != len(self.UpBlocks):\n",
    "      raise RuntimeError('Mismatched number of blocks')\n",
    "  \n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: input of shape (N, 3, W, H)\n",
    "    #forward pass through downsampling blocks\n",
    "    pooling_params = []\n",
    "    for d in self.DownBlocks:\n",
    "      (x, indices, W, H) = d(x)\n",
    "      pooling_params.insert(0, (indices, W, H))\n",
    "    \n",
    "    #forward pass through upsampling blocks\n",
    "    for i in range(len(pooling_params)):\n",
    "      x = self.UpBlocks[i](x, pooling_params[i])\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0nEBIxtJ3CEo"
   },
   "outputs": [],
   "source": [
    "# A dictionary of different SegNet network parameters\n",
    "networks = {\n",
    "    'first_net': {           # My implementation, based off of SegNet but with fewer layers/weights\n",
    "        'DownBlocks':[       # Achieved ~98 val accuracy in 20 epochs on the Adam optimizer, batch_size=100\n",
    "                             # ~116,342 parameters\n",
    "                {\n",
    "                    'channels_list' : [3, 6, 10],  #Remider: there are 2 ConvLayers here\n",
    "                    'kernel_size' : 7,             #A (3->6) layer and a (6->10) layer\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [10, 10, 15],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [15, 15, 32],  \n",
    "                    'kernel_size' : 5,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [32, 32, 64],  \n",
    "                    'kernel_size' : 5,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 128],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                }\n",
    "        ],\n",
    "        'UpBlocks':[\n",
    "                    {\n",
    "                    'channels_list' : [128, 64, 64],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 32, 32],  \n",
    "                    'kernel_size' : 5,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [32, 15, 15],  \n",
    "                    'kernel_size' : 5,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [15, 10, 10],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [10, 6, 2],  #The final channels_out represents the possible classes\n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "\n",
    "    'VGG16' : {           # A structure more closely resembling VGG16, which is what the original SegNet uses\n",
    "        'DownBlocks': [   # Unfortunately I keep running out of memory before I can use it.\n",
    "                {    \n",
    "                    'channels_list' : [3, 64, 64],  #Remider: there are 2 ConvLayers here;\n",
    "                    'kernel_size' : 3,              #A (3->64) layer and a (64->64) layer\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 128, 128],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [128, 256, 256, 256],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [256, 512, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [512, 512, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                }\n",
    "        ],\n",
    "        'UpBlocks':[\n",
    "                    {\n",
    "                    'channels_list' : [512, 512, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [512, 512, 512, 256],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [256, 256, 256, 128],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [128, 128, 64],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 2],  #The final channels_out represents the possible classes\n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "\n",
    "\n",
    "    'VGG11' : {           # Like VGG16, but 5 fewer Conv layers and fewer weights\n",
    "        'DownBlocks': [   # Also had memory issues with this one\n",
    "                {    \n",
    "                    'channels_list' : [3, 64],  \n",
    "                    'kernel_size' : 3,              \n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 128],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [128, 256, 256],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [256, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [512, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'pooling_factor' : 2\n",
    "                }\n",
    "        ],\n",
    "        'UpBlocks':[\n",
    "                    {\n",
    "                    'channels_list' : [512, 512, 512],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [512, 512, 256],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [256, 256, 128],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [128, 64],  \n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 2],  #The final channels_out represents the possible classes\n",
    "                    'kernel_size' : 3,\n",
    "                    'unpooling_factor' : 2\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "\n",
    "\n",
    "    'SG_Basic' : {        # Based on an earler version of SegNet, called SegNet_Basic\n",
    "        'DownBlocks': [   # The original SGB doesn't use ReLU's, unlike this one\n",
    "                {         # Achieved 97.8% val acc in 20 epochs using SGD, batch_size=100\n",
    "                    'channels_list' : [3, 64],  \n",
    "                    'kernel_size' : 7,              \n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                }\n",
    "        ],\n",
    "        'UpBlocks':[\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 2],  #The final channels_out represents the possible classes\n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "\n",
    "\n",
    "     'SGBv2' : {          # Like SegNet_Basic, but with more parameters and layers\n",
    "        'DownBlocks': [   \n",
    "                          # ~461,440 parameters\n",
    "                {    \n",
    "                    'channels_list' : [3, 64],  \n",
    "                    'kernel_size' : 7,              \n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'pooling_factor' : 2\n",
    "                }\n",
    "                \n",
    "        ],\n",
    "        'UpBlocks':[\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 64],  \n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                },\n",
    "                {\n",
    "                    'channels_list' : [64, 2],  #The final channels_out represents the possible classes\n",
    "                    'kernel_size' : 7,\n",
    "                    'unpooling_factor' : 2\n",
    "                }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCQqPHuf_EeL"
   },
   "source": [
    "# Training the Net\n",
    "The training net is stored in net_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SCNCH9KJj8JJ"
   },
   "outputs": [],
   "source": [
    "#Borrowed from homework2 (modified)\n",
    "# Checks the accuracy of the model on either the validation set or the training set\n",
    "# accuracy is the percentage of pixels over all samples that were classified correctly\n",
    "def check_accuracy(model, mode='val'):\n",
    "    if mode == 'val':\n",
    "        print('Checking accuracy on validation set')\n",
    "        x = x_val\n",
    "        y = y_val\n",
    "    elif mode == 'test':\n",
    "        print('Checking accuracy on test set')   \n",
    "        x = x_test\n",
    "        y = y_test\n",
    "    else:\n",
    "      raise AttributeError('Bad accuracy check mode')\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        x = x.to(torch.device('cuda:0'))\n",
    "        y = y.to(torch.device('cuda:0'))\n",
    "\n",
    "        scores = model(x)\n",
    "        _, preds = scores.max(1)\n",
    "\n",
    "        total = 1\n",
    "        for s in preds.shape:\n",
    "          total = total * s\n",
    "\n",
    "        correct_preds = torch.zeros_like(preds)\n",
    "        correct_preds[preds==y] = 1\n",
    "        num_correct = correct_preds.sum()\n",
    "\n",
    "        acc = float(num_correct)/total\n",
    "\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, total, 100 * acc))\n",
    "        loss = torch.nn.functional.cross_entropy(scores, y.long())\n",
    "\n",
    "        x = x.to(torch.device('cpu'))\n",
    "        y = y.to(torch.device('cpu'))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N61RXpV_VoiU"
   },
   "outputs": [],
   "source": [
    "#Borrowed from homework2 (modified)\n",
    "# Trains the given model with the given optimizer\n",
    "def train_model(model, optimizer, max_batch_size=100, epochs=1, print_every=10):\n",
    "    \"\"\"  \n",
    "    Inputs:\n",
    "    - model:           A PyTorch Module giving the model to train.\n",
    "    - optimizer:       An Optimizer object we will use to train the model\n",
    "    - max_batch_size:  Maximum minibatch size\n",
    "    - epochs:          (Optional) A Python integer giving the number of epochs to train for\n",
    "    - print_every:     Number of GD steps between each printed update\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    total_data = x_train.shape[0]\n",
    "    iter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        batch_index = 0\n",
    "        #shuffle data before starting each epoch \n",
    "        (x_shuffled, y_shuffled) = shuffle_two_arrays(x_train, y_train)\n",
    "\n",
    "        while batch_index < total_data:\n",
    "            batch_end = batch_index + max_batch_size\n",
    "            if batch_end > total_data:\n",
    "              batch_end = total_data\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            x = x_shuffled[batch_index:batch_end, :, :, :]\n",
    "            y = y_shuffled[batch_index:batch_end, :, :]\n",
    "\n",
    "            # Put data onto the cuda\n",
    "            x = x.to(torch.device('cuda:0'))\n",
    "            y = y.to(torch.device('cuda:0'))\n",
    "\n",
    "            # Run model and calculate loss\n",
    "            scores = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(scores, y.long())\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print('Epoch %d, GD Iteration %d, loss = %.4f' % (e, iter, loss.item()))\n",
    "                val_loss = check_accuracy(model, mode='val')\n",
    "                val_losses.append(val_loss)\n",
    "                print()\n",
    "\n",
    "            # Take the data off the cuda\n",
    "            x = x.to(torch.device('cpu'))\n",
    "            y = y.to(torch.device('cpu'))\n",
    "\n",
    "            batch_index = batch_end\n",
    "            iter = iter + 1\n",
    "    \n",
    "    plt.plot(range(iter), train_losses, label='training loss')\n",
    "    plt.plot(np.linspace(0, iter, len(val_losses)), val_losses, label='validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IGuvEPvNpeOd"
   },
   "outputs": [],
   "source": [
    "# a dictionary of optimizer params\n",
    "optim_params = {\n",
    "    'Adam' : {\n",
    "        'optimizer' : optim.Adam,\n",
    "        'params' : (1e-3,(0.9,0.999))\n",
    "    },\n",
    "\n",
    "    'SGD' : {  # Optimizer used to train the original SegNet\n",
    "        'optimizer' : optim.SGD,\n",
    "        'params' : (0.1, 0.9)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rA0NRUwc_XEw",
    "outputId": "f4fe36b2-c0d8-4487-ccfc-6e716fd37f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, GD Iteration 0, loss = 0.7662\n",
      "Checking accuracy on validation set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.56 GiB (GPU 0; 6.00 GiB total capacity; 3.86 GiB already allocated; 533.62 MiB free; 3.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/2558816384.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SGD'\u001b[0m\u001b[1;33m]\u001b[0m                     \u001b[1;31m#Pick the optimizer to use from the optim_params dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_to_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_to_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/857702082.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, max_batch_size, epochs, print_every)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch %d, GD Iteration %d, loss = %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/1330748417.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/3766540682.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mpooling_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDownBlocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m       \u001b[0mpooling_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/172625515.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#         indices = (N, C, W, H)? shouldn't need to deal with size here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#         (W_in, H_in)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6808/2232211878.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"\n\u001b[1;32m--> 168\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2282\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.56 GiB (GPU 0; 6.00 GiB total capacity; 3.86 GiB already allocated; 533.62 MiB free; 3.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "   Train the net here\n",
    "\n",
    "'''\n",
    "net_to_train = OurSegNet(networks['SGBv2'])  #Pick the net to train from the networks dictionary\n",
    "tp = optim_params['SGD']                     #Pick the optimizer to use from the optim_params dictionary\n",
    "\n",
    "train_model(net_to_train, tp['optimizer'](net_to_train.parameters(), *tp['params']), epochs=20, max_batch_size=32, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKYPKS_C_ajU"
   },
   "source": [
    "# Finding the Bounding Box\n",
    "These methods are used to take in a single segmentation mask and return one or more bounding boxes that represent the masks.\n",
    "\n",
    "Created by Hunter Jensen, with minor edits to fit this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHTVcsQ4BJ4V"
   },
   "outputs": [],
   "source": [
    "def displayBGR(image):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayGRAY(image):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_GRAY2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObDlANNCBCzl"
   },
   "outputs": [],
   "source": [
    "# This function will create a line that starts at (x1, y1) and pass through (x2,y2), continuing to\n",
    "# the boundaries of the image, which are defined by 0 and maxX and maxY.  Returns an array of points\n",
    "# (pixels) on that line.\n",
    "def get_line_long(x1, y1, x2, y2, maxX, maxY):\n",
    "    minX = 0\n",
    "    minY = 0\n",
    "\n",
    "    points = []\n",
    "    issteep = abs(y2 - y1) > abs(x2 - x1)\n",
    "    if issteep:\n",
    "        x1, y1 = y1, x1\n",
    "        x2, y2 = y2, x2\n",
    "    rev = False\n",
    "    if x1 > x2:\n",
    "        x1 = -x1    # Invert x and y so line is always going in positive direction\n",
    "        x2 = -x2\n",
    "        y1 = -y1\n",
    "        y2 = -y2\n",
    "        rev = True\n",
    "    deltax = x2 - x1\n",
    "    deltay = abs(y2 - y1)\n",
    "    error = int(deltax / 2)\n",
    "    y = y1\n",
    "    ystep = None\n",
    "    if y1 < y2:\n",
    "        ystep = 1\n",
    "    else:\n",
    "        ystep = -1\n",
    "\n",
    "    complete = False\n",
    "    x = x1  # Start at x1\n",
    "    while not complete:\n",
    "        # if not rev:\n",
    "        if issteep:\n",
    "            if not rev:\n",
    "                points.append((y, x))\n",
    "                if (x <= minY) or (x >= maxY) or (y <= minX) or (y >= maxX):\n",
    "                    complete = True\n",
    "            elif rev:\n",
    "                points.append((-y, -x))\n",
    "                if (x >= -minY) or (x <= -maxY) or (y >= -minX) or (y <= -maxX):\n",
    "                    complete = True\n",
    "        else:\n",
    "            if not rev:\n",
    "                points.append((x, y))\n",
    "                if (x <= minX) or (x >= maxX) or (y <= minY) or (y >= maxY):\n",
    "                    complete = True\n",
    "            elif rev:\n",
    "                points.append((-x, -y))\n",
    "                if (x >= -minX) or (x <= -maxX) or (y >= -minY) or (y <= -maxY):\n",
    "                    complete = True\n",
    "        error -= deltay\n",
    "        if error < 0:\n",
    "            y += ystep\n",
    "            error += deltax\n",
    "        x += 1  # Increment x\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBMuMOy_A_V2"
   },
   "outputs": [],
   "source": [
    "def get_line(x1, y1, x2, y2):\n",
    "    points = []\n",
    "    issteep = abs(y2 - y1) > abs(x2 - x1)\n",
    "    if issteep:\n",
    "        x1, y1 = y1, x1\n",
    "        x2, y2 = y2, x2\n",
    "    rev = False\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1\n",
    "        y1, y2 = y2, y1\n",
    "        rev = True\n",
    "    deltax = x2 - x1\n",
    "    deltay = abs(y2 - y1)\n",
    "    error = int(deltax / 2)\n",
    "    y = y1\n",
    "    ystep = None\n",
    "    if y1 < y2:\n",
    "        ystep = 1\n",
    "    else:\n",
    "        ystep = -1\n",
    "    for x in range(x1, x2 + 1):\n",
    "        if issteep:\n",
    "            points.append((y, x))\n",
    "        else:\n",
    "            points.append((x, y))\n",
    "        error -= deltay\n",
    "        if error < 0:\n",
    "            y += ystep\n",
    "            error += deltax\n",
    "    # Reverse the list if the coordinates were reversed\n",
    "    if rev:\n",
    "        points.reverse()\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPJFl53OA6zO"
   },
   "outputs": [],
   "source": [
    "# (X[i], Y[i]) are coordinates of i'th point.\n",
    "def polygonArea(X, Y):\n",
    "    n = len(X)\n",
    "    # Initialize area\n",
    "    area = 0.0\n",
    "\n",
    "    # Calculate value of shoelace formula\n",
    "    j = n - 1\n",
    "    for i in range(0, n):\n",
    "        area += (X[j] + X[i]) * (Y[j] - Y[i])\n",
    "        j = i  # j is previous vertex to i\n",
    "\n",
    "    # Return absolute value\n",
    "    return abs(area / 2.0)\n",
    "\n",
    "\n",
    "\n",
    "# Just calls polygonArea\n",
    "def area_of_box(box):\n",
    "    return polygonArea((box[0][0], box[1][0], box[2][0], box[3][0]), (box[0][1], box[1][1], box[2][1], box[3][1]))\n",
    "\n",
    "\n",
    "\n",
    "# Like area_of_box but so it can easily be called to check a new line\n",
    "# Side is the side you're working on, point1 and point2 are the new points you're testing,\n",
    "# box is the original box that you're replacing one edge of\n",
    "def area_of_new_box(side, point1, point2, box):\n",
    "    # IMPORTANT: point2 must follow point1 clockwise\n",
    "    if side == 'left':\n",
    "        return polygonArea((point1[0], point2[0], box[2][0], box[3][0]), (point1[1], point2[1], box[2][1], box[3][1]))\n",
    "    elif side == 'top':\n",
    "        return polygonArea((box[0][0], point1[0], point2[0], box[3][0]), (box[0][1], point1[1], point2[1], box[3][1]))\n",
    "    elif side == 'right':\n",
    "        return polygonArea((box[0][0], box[1][0], point1[0], point2[0]), (box[0][1], box[1][1], point1[1], point2[1]))\n",
    "    elif side == 'bottom':\n",
    "        return polygonArea((point2[0], box[1][0], box[2][0], point1[0]), (point2[1], box[1][1], box[2][1], point1[1]))\n",
    "    else:\n",
    "        exit('Error: bad side in area_of_new_box: ' + str(side))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTEdRmfOA1SF"
   },
   "outputs": [],
   "source": [
    "# Image passed in must be GRAY, not BGR\n",
    "# Returns false if no conflicts\n",
    "def check_box_for_conflicts(box, img):\n",
    "    left_side = get_line(box[0][0], box[0][1], box[1][0], box[1][1])\n",
    "    top_side = get_line(box[1][0], box[1][1], box[2][0], box[2][1])\n",
    "    right_side = get_line(box[2][0], box[2][1], box[3][0], box[3][1])\n",
    "    bottom_side = get_line(box[3][0], box[3][1], box[0][0], box[0][1])\n",
    "    all_sides = left_side + top_side + right_side + bottom_side\n",
    "    for pix in all_sides:\n",
    "        if img[pix[1]][pix[0]] != 0:\n",
    "            # print(\"Problem: \" + str(pix[1]) + \",\" + str(pix[0]]))\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Like check_box_for_conflicts, but checks one line (pass in two points)\n",
    "# Image passed in must be GRAY, not BGR\n",
    "# Returns false if no conflicts\n",
    "def check_line_for_conflicts(point1, point2, img):\n",
    "    line = get_line(point1[0], point1[1], point2[0], point2[1])\n",
    "    for pix in line:\n",
    "        if img[pix[1]][pix[0]] != 0:\n",
    "            # print(\"Problem: \" + str(pix[1]) + \",\" + str(pix[0]]))\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4F7q2g3AyD-"
   },
   "outputs": [],
   "source": [
    "# First pushes the edge out until it doesn't conflict, then rotates right and then left to find minimum area\n",
    "def rotateCaliperLine(side, top_line, bottom_line, starting_top_point, starting_bottom_point, box, img):\n",
    "    min_area = 999999999  # This will contain the minimum area of the box\n",
    "    min_points = None  # This will be an array of points that give the min_area\n",
    "\n",
    "    # Display the box before rotation (optional)\n",
    "    # img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    # cv2.drawContours(img_bgr, [box], 0, (0, 0, 255), 1)\n",
    "    # displayBGR(img_bgr)\n",
    "\n",
    "    # Check starting point\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        # If there are conflicts\n",
    "        try:\n",
    "            if check_line_for_conflicts(top_line[starting_top_point], bottom_line[starting_bottom_point], img):\n",
    "                # If so, push back and repeat\n",
    "                starting_top_point += 1\n",
    "                starting_bottom_point += 1\n",
    "                # If can't push back further, you'll error out in the except block\n",
    "            # Once you're not conflicting get your starting min_area\n",
    "            else:\n",
    "                point1 = top_line[starting_top_point]\n",
    "                point2 = bottom_line[starting_bottom_point]\n",
    "                min_area = area_of_new_box(side, point1, point2, box)\n",
    "                min_points = [point1, point2]\n",
    "                complete = True\n",
    "        except IndexError:\n",
    "            exit(\"ERROR: for side \" + str(\n",
    "                side) + \" we couldn't find a starting line that didn't conflict and was in bounds\")\n",
    "    # print('* Starting point: ' + str(top_line[starting_top_point]) + \" \" + str(bottom_line[starting_bottom_point]) + \" *\")\n",
    "\n",
    "    # Display output lines! (Optional)\n",
    "    # img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    # cv2.line(img_bgr, min_points[0], min_points[1], (0, 255, 0), 1)\n",
    "    # displayBGR(img_bgr)\n",
    "\n",
    "    # Then rotate right (by decrementing the bottom)\n",
    "    #    /------/ | ->\n",
    "    #   /------/  |\n",
    "    #  /------/   |\n",
    "    # /______/    | <-\n",
    "    top_point = starting_top_point\n",
    "    bottom_point = starting_bottom_point\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        try:\n",
    "            bottom_point -= 1  # Decrement the bottom point\n",
    "            # Shift right until you don't conflict\n",
    "            while check_line_for_conflicts(top_line[top_point], bottom_line[bottom_point], img):\n",
    "                top_point += 1\n",
    "                bottom_point += 1\n",
    "            # Now that you don't conflict, see if it's a new minimum area!\n",
    "            point1 = top_line[top_point]\n",
    "            point2 = bottom_line[bottom_point]\n",
    "            if area_of_new_box(side, point1, point2, box) <= min_area:  # FIXME: <= or < ?\n",
    "                min_area = area_of_new_box(side, point1, point2, box)\n",
    "                min_points = [point1, point2]\n",
    "                # print(\"New min points: \" + str(min_points))\n",
    "        except IndexError:\n",
    "            # Go until your top_point goes out of bounds\n",
    "            complete = True\n",
    "\n",
    "    # Go back to start and rotate left\n",
    "    top_point = starting_top_point\n",
    "    bottom_point = starting_bottom_point\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        try:\n",
    "            top_point -= 1  # Decrement the *TOP* point\n",
    "            # Shift right until you don't conflict\n",
    "            while check_line_for_conflicts(top_line[top_point], bottom_line[bottom_point], img):\n",
    "                top_point += 1\n",
    "                bottom_point += 1\n",
    "            # Now that you don't conflict, see if it's a new minimum area!\n",
    "            point1 = top_line[top_point]\n",
    "            point2 = bottom_line[bottom_point]\n",
    "            if area_of_new_box(side, point1, point2, box) <= min_area:  # FIXME: <= or < ?\n",
    "                min_area = area_of_new_box(side, point1, point2, box)\n",
    "                min_points = [point1, point2]\n",
    "                # print(\"New min points: \" + str(min_points))\n",
    "        except IndexError:\n",
    "            # Go until your top_point goes out of bounds\n",
    "            complete = True\n",
    "\n",
    "    # Display output lines! (Optional)\n",
    "    # img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    # cv2.line(img_bgr, min_points[0], min_points[1], (0, 255, 0), 1)\n",
    "    # displayBGR(img_bgr)\n",
    "\n",
    "    return min_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hOBT_aCArk-"
   },
   "outputs": [],
   "source": [
    "# Takes in which side of box to work with.  It rotates that side as far as it can to both degrees,\n",
    "# optimizing for minimum box area\n",
    "def rotateCaliper(side, box, img):\n",
    "    top_line = None  # Will be an array of points\n",
    "    bottom_line = None  # Will be an array of points\n",
    "    relative_top_idx = None\n",
    "    relative_bottom_idx = None\n",
    "    rel_top_far_idx = None  # The index of far top corner\n",
    "    rel_bot_far_idx = None  # Index of far bottom corner\n",
    "\n",
    "    if side == 'right':\n",
    "\n",
    "        relative_top_idx = 2\n",
    "        relative_bottom_idx = 3\n",
    "        rel_top_far_idx = 1\n",
    "        rel_bot_far_idx = 0\n",
    "    elif side == 'bottom':\n",
    "        relative_top_idx = 3\n",
    "        relative_bottom_idx = 0\n",
    "        rel_top_far_idx = 2\n",
    "        rel_bot_far_idx = 1\n",
    "    elif side == 'left':\n",
    "        relative_top_idx = 0\n",
    "        relative_bottom_idx = 1\n",
    "        rel_top_far_idx = 3\n",
    "        rel_bot_far_idx = 2\n",
    "    elif side == 'top':\n",
    "        relative_top_idx = 1\n",
    "        relative_bottom_idx = 2\n",
    "        rel_top_far_idx = 0\n",
    "        rel_bot_far_idx = 3\n",
    "    else:\n",
    "        exit(\"In function rotateCaliper, the input side is not recognized: \" + str(side))\n",
    "\n",
    "    # Use get_line_long to get top and bottom line:\n",
    "    top_line = get_line_long(box[rel_top_far_idx][0], box[rel_top_far_idx][1], box[relative_top_idx][0], box[relative_top_idx][1], img.shape[1]-1, img.shape[0]-1)\n",
    "    bottom_line = get_line_long(box[rel_bot_far_idx][0], box[rel_bot_far_idx][1], box[relative_bottom_idx][0], box[relative_bottom_idx][1], img.shape[1]-1, img.shape[0]-1)\n",
    "\n",
    "    # FIXME: Remove this try block later once you know it's always true\n",
    "    try:  # Ensure that the arrays are in the right direction\n",
    "        top_closePoint = top_line.index((box[relative_top_idx][0], box[relative_top_idx][1]))\n",
    "        top_farPoint = top_line.index((box[rel_top_far_idx][0], box[rel_top_far_idx][1]))\n",
    "        bot_closePoint = bottom_line.index((box[relative_bottom_idx][0], box[relative_bottom_idx][1]))\n",
    "        bot_farPoint = bottom_line.index((box[rel_bot_far_idx][0], box[rel_bot_far_idx][1]))\n",
    "        if top_closePoint < top_farPoint:\n",
    "            top_line.reverse()  # Reverse the array so that incrementing the index will go away from our close point\n",
    "            print(\"REVERSING TOP ARRAY\")\n",
    "        if bot_closePoint < bot_farPoint:\n",
    "            bottom_line.reverse()  # Do the same for bottom array as well\n",
    "            print(\"REVERSING BOTTOM ARRAY\")\n",
    "    except ValueError:\n",
    "        # This always fails when one of the points is outside - of the box, FIXME later?\n",
    "        # exit(\"ERROR: Point not found on either top or bottom line...\")\n",
    "        print(\"ERROR: Point not found within bounds\")\n",
    "        return\n",
    "\n",
    "    starting_top_point = top_line.index((box[relative_top_idx][0], box[relative_top_idx][1]))\n",
    "    starting_bottom_point = bottom_line.index((box[relative_bottom_idx][0], box[relative_bottom_idx][1]))\n",
    "\n",
    "    min_points = rotateCaliperLine(side, top_line, bottom_line, starting_top_point, starting_bottom_point, box, img)\n",
    "\n",
    "    # Update box with new minimum area points\n",
    "    box[relative_top_idx][0] = min_points[0][0]\n",
    "    box[relative_top_idx][1] = min_points[0][1]\n",
    "    box[relative_bottom_idx][0] = min_points[1][0]\n",
    "    box[relative_bottom_idx][1] = min_points[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJgSTBpLAlym"
   },
   "outputs": [],
   "source": [
    "# Will minimize box around a non-black object in img\n",
    "# Works by attempting to rotate each edge for minimal area\n",
    "def geometricMinimizeQuad(box, img):\n",
    "    loops = 3\n",
    "    min_box = np.copy(box)\n",
    "    for i in range(loops):\n",
    "        rotateCaliper('right', min_box, img)\n",
    "        rotateCaliper('bottom', min_box, img)\n",
    "        rotateCaliper('left', min_box, img)\n",
    "        rotateCaliper('top', min_box, img)\n",
    "    return min_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kTHa12wAeB_"
   },
   "outputs": [],
   "source": [
    "# img must be GRAY, not BGR\n",
    "# This implements an algorithm to shrink the bounding box around a non-black object in img\n",
    "# Currently calls geometricMinimizeQuad, originally it called recursive_min_box but it wasn't\n",
    "# very efficient so that method is not recommended.\n",
    "def minEnclosingQuad(cnt, img, show_before=False, show_after=False):\n",
    "    # Fill in contour of picture with white\n",
    "    cv2.fillPoly(img, pts=[cnt], color=(255, 255, 255))\n",
    "\n",
    "    # Get coordinates\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "\n",
    "    if show_before:\n",
    "        # Display the box (optional)\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.drawContours(img_bgr, [box], 0, (0, 0, 255), 1)\n",
    "        displayBGR(img_bgr)\n",
    "\n",
    "    minBox = geometricMinimizeQuad(box, img)\n",
    "    # print(\"done!\")\n",
    "\n",
    "    if show_after:\n",
    "        # Display the box (optional)\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.drawContours(img_bgr, [minBox], 0, (0, 0, 255), 1)\n",
    "        displayBGR(img_bgr)\n",
    "\n",
    "    return minBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzZfdu7-_-t7"
   },
   "outputs": [],
   "source": [
    "def getMinQuad(img, max_quads=10, value=210, show_before=False, show_after=False):\n",
    "    height, width, _ = img.shape\n",
    "    border = round(max(height, width)/2)\n",
    "\n",
    "    hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hsv_lower = np.array([0, 0, value], dtype=\"uint8\")\n",
    "    hsv_upper = np.array([255, 255, 255], dtype=\"uint8\")\n",
    "    # find the colors within the specified boundaries and apply the mask\n",
    "    mask = cv2.inRange(hsv_image, hsv_lower, hsv_upper)\n",
    "    if show_before:\n",
    "        plt.imshow(mask)\n",
    "        plt.show()\n",
    "    # Make image larger, add in border around each side\n",
    "    mask = cv2.copyMakeBorder(mask, border, border, border, border, cv2.BORDER_CONSTANT, (0,0,0))\n",
    "\n",
    "    contours = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = imutils.grab_contours(contours)\n",
    "\n",
    "    # Adjust num_quads so that it isn't larger than the number of contors\n",
    "    if max_quads > len(contours):\n",
    "      num_quads = len(contours)\n",
    "    else:\n",
    "      num_quads = max_quads\n",
    "    \n",
    "    contour = sorted(contours, key=cv2.contourArea, reverse=True)[:num_quads]  # Only take the biggest contour\n",
    "    min_quads = np.zeros((num_quads, 4, 2), dtype=int)\n",
    "\n",
    "    for i, cnt in enumerate(contour):\n",
    "        min_quad = minEnclosingQuad(cnt, mask, show_before=show_before, show_after=show_after)\n",
    "        min_quad = min_quad - border\n",
    "        min_quads[i] = min_quad\n",
    "\n",
    "    if show_after:\n",
    "        cv2.drawContours(img, [min_quads[0]], 0, (0, 0, 255), 2)    # Showing more than one throws errors because contours are technically lists of arrays, not a 3d numpy array\n",
    "        displayBGR(img)\n",
    "\n",
    "    return min_quads    # Shape [num_quads, 4, 2]\n",
    "\n",
    "\n",
    "# Example call: \n",
    "# filename = \"003526.jpe\"\n",
    "# image = cv2.imread(filename)\n",
    "# mq = getMinQuad(image, 3)\n",
    "# print(mq)   # Should be shape [3, 4, 2] since we asked for 3 cards in the previous line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0klQKzUSGup7"
   },
   "source": [
    "# Displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jxKll7jIGgZ"
   },
   "outputs": [],
   "source": [
    "# Takes in a mask (W, H) or (1, W, H) and returns an image (W, H, 3) \n",
    "# If mask is a pytorch tensor, call mask.to(torch.device('cpu')) first\n",
    "def mask_to_rgb(mask):\n",
    "  W, H = mask.shape[-2:]\n",
    "  img = np.zeros((W, H, 3), dtype=np.uint8)\n",
    "  img[:, :, 0] = mask\n",
    "  img[img != 0] = 255\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERv0sBhmq_pq"
   },
   "outputs": [],
   "source": [
    "# Run a single image through the net and display the results\n",
    "# Inputs:\n",
    "# img         - an ndarray (C, W, H) representing a single image from all_imgs\n",
    "# label       - the corresponding label (W, H), from all_labels\n",
    "# model       - the model to run the image on\n",
    "# showCorrect - If true, also shows the correct segmentation (separately)\n",
    "# show_img    - If true, layers the segmentation on top of the original img\n",
    "#               If false, just shows the mask by itself\n",
    "def display_example_segment(img, label, model, showCorrect=False, show_img=True):\n",
    "  net_input = numpy_normalize_rgb(img.reshape(1, *img.shape))\n",
    "  net_output = model(torch.tensor(net_input).to(torch.device('cuda:0'), torch.float32))\n",
    "  img = np.moveaxis(img, 0, -1)\n",
    "\n",
    "  if(showCorrect):\n",
    "    print('Correct segmentation:')\n",
    "    correct_mask = mask_to_rgb(label)\n",
    "    if show_img:\n",
    "      plt.imshow(img)\n",
    "    plt.imshow(correct_mask, alpha=0.75)\n",
    "    plt.show()\n",
    "\n",
    "  print('SegNet segmentation:')\n",
    "  _, preds = net_output.max(1)\n",
    "  net_mask = mask_to_rgb(preds.to(torch.device('cpu')))\n",
    "  if show_img:\n",
    "    plt.imshow(img)\n",
    "  plt.imshow(net_mask, alpha=0.75)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhZUMDBqKXWq"
   },
   "outputs": [],
   "source": [
    "# Takes in an img from all_imgs, runs it through the net,\n",
    "# and displays the resulting bounding box\n",
    "def display_example_bound(img, model, max_boxes=1):\n",
    "  net_input = numpy_normalize_rgb(img.reshape(1, *img.shape))\n",
    "  net_output = model(torch.tensor(net_input).to(torch.device('cuda:0'), torch.float32))\n",
    "  img = np.moveaxis(img, 0, -1)\n",
    "  _, preds = net_output.max(1)\n",
    "  quads = getMinQuad(mask_to_rgb(preds.to(torch.device('cpu'))), max_quads=max_boxes) # shape (num_cards, 4, 2)\n",
    "  print(str(max_boxes), 'Approximate bounding boxes:')\n",
    "  displayRGB(img, quads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ju4sasoFPfYM",
    "outputId": "6ac642c1-9127-4c96-d899-e2590b732ea1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "   Visualize some example segmentation and bounding box output here\n",
    "  \n",
    "   Toggle show_img if you want to see the segmentation layered on top\n",
    "   of the original image\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# displays some of the images and their results\n",
    "# index is the image's index in all_imgs, which is compiled in the compile_data function\n",
    "total = len(all_imgs)\n",
    "for i in range(5):\n",
    "  index = round(random.uniform(0, total))\n",
    "  # index = total - i - 30\n",
    "  print('\\nindex =',index)\n",
    "  display_example_segment(all_imgs[index], all_labels[index], net_to_train, show_img=False) # Shows segmentation results\n",
    "  display_example_bound(all_imgs[index], net_to_train, max_boxes=1)                         # Shows the approximte bounding boxes of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0wbA3LQanNM"
   },
   "outputs": [],
   "source": [
    "def check_single_accuracy(pred, correct, max_dist=2):\n",
    "  \"\"\"\n",
    "  Compute the accuracy one classification.  Pred and correct are numpy arrays\n",
    "  of shape [1, 4, 2].  Returns 1 if it was correct (worst corner within max_dist)\n",
    "  and 0 otherwise\n",
    "  \"\"\"\n",
    "  diff = correct - pred\n",
    "  diff = np.all(np.all(diff<=max_dist, axis=2),axis=1)\n",
    "  num_correct = np.sum(diff)\n",
    "  return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfaqW0ViCKBm",
    "outputId": "f4a5b946-2434-4566-f1e7-810251f45605"
   },
   "outputs": [],
   "source": [
    "x_full, y_full = read_images(max_samples = 200, cards_per_image=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPLAga0DtPc"
   },
   "outputs": [],
   "source": [
    "def reorder_coords(coords):\n",
    "  '''\n",
    "  Takes in the coordinates for 1 cards and rolls it so that they are in the right\n",
    "  order (The minimum point, aka top left, should be at index 1)\n",
    "  '''\n",
    "  min_matrix = np.sum(coords, axis=2)\n",
    "  roll_num = -(np.argmin(min_matrix) - 1)    # The minimum should be in position 1 (since order is bottom left, top left, top right, bottom right)\n",
    "  new_coords = np.roll(coords, roll_num, axis=1)\n",
    "  return new_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKaVFAMB-zqB"
   },
   "outputs": [],
   "source": [
    "def check_box_accuracy(x, y, max_dist=2):\n",
    "  '''\n",
    "  Checks the accuracy of the estimated bounding boxes\n",
    "  '''\n",
    "  num_correct = 0\n",
    "  num_samples = x.shape[0]\n",
    "  for i in range(num_samples):\n",
    "    if (i%50 == 0) and (i != 0):\n",
    "      print(i)\n",
    "    img = x[i]\n",
    "    coords = y[i]\n",
    "    net_input = numpy_normalize_rgb(img.reshape(1, *img.shape))\n",
    "    net_output = net_to_train(torch.tensor(net_input).to(torch.device('cuda:0'), torch.float32))\n",
    "    _, preds = net_output.max(1)\n",
    "    quads = getMinQuad(mask_to_rgb(preds.to(torch.device('cpu'))), max_quads=1) # shape (num_cards, 4, 2)\n",
    "    correct = check_single_accuracy(reorder_coords(quads), coords, max_dist=max_dist)\n",
    "    num_correct += correct\n",
    "  acc = num_correct/num_samples\n",
    "  print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY8MwuVOK5ED",
    "outputId": "ba40663a-f3d7-4ca2-e398-615efaa0c3bc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Resulting bounding box accuracy shown here,\n",
    "for different pixel distances.\n",
    "\n",
    "'''\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=5)\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=4)\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=3)\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=2)\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=1)\n",
    "acc = check_box_accuracy(x_full, y_full, max_dist=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oKYPKS_C_ajU"
   ],
   "name": "Segmentation - Final",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
